{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Autograded Notebook (Canvas & CodeGrade)\n",
    "\n",
    "This notebook will be automatically graded. It is designed to test your answers and award points for the correct answers. Following the instructions for each Task carefully.\n",
    "Instructions\n",
    "\n",
    "- **Download** this notebook as you would any other ipynb file \n",
    "- **Upload** to Google Colab or work locally (if you have that set-up)\n",
    "- **Delete** `raise NotImplementedError()`\n",
    "\n",
    "- **Write** your code in the `# YOUR CODE HERE` space\n",
    "\n",
    "\n",
    "- **Execute** the Test cells that contain assert statements - these help you check your work (others contain hidden tests that will be checked when you submit through Canvas)\n",
    "\n",
    "- **Save** your notebook when you are finished\n",
    "- **Download** as a ipynb file (if working in Colab)\n",
    "- **Upload** your complete notebook to Canvas (there will be additional instructions in Slack and/or Canvas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint Challenge\n",
    "## *Data Science Unit 4 Sprint 1*\n",
    "\n",
    "After a week of Natural Language Processing, you've learned some cool new stuff: how to process text, how turn text into vectors, and how to model topics from documents. Apply your newly acquired skills to one of the most famous NLP datasets out there: [Yelp](https://www.yelp.com/dataset). As part of the job selection process, some of my friends have been asked to create analysis of this dataset, so I want to empower you to have a head start.  \n",
    "\n",
    "The real dataset is massive (almost 8 gigs uncompressed). I've sampled the data for you to something more managable for the Sprint Challenge. You can analyze the full dataset as a stretch goal or after the sprint challenge. As you work on the challenge, I suggest adding notes about your findings and things you want to analyze in the future.\n",
    "\n",
    "## Challenge Objectives\n",
    "Successfully complete all these objectives to earn full credit. \n",
    "\n",
    "**Successful completion is defined as passing all the unit tests in each objective.**  \n",
    "\n",
    "Each unit test that you pass is 1 point. \n",
    "\n",
    "There are 5 total possible points in this sprint challenge. \n",
    "\n",
    "\n",
    "There are more details on each objective further down in the notebook.*\n",
    "* <a href=\"#p1\">Part 1</a>: Write a function to tokenize the yelp reviews\n",
    "* <a href=\"#p2\">Part 2</a>: Create a vector representation of those tokens\n",
    "* <a href=\"#p3\">Part 3</a>: Use your tokens in a classification model on yelp rating\n",
    "* <a href=\"#p4\">Part 4</a>: Estimate & Interpret a topic model of the Yelp reviews\n",
    "\n",
    "____\n",
    "\n",
    "# Before you submit your notebook you must first\n",
    "\n",
    "1) Restart your notebook's Kernal\n",
    "\n",
    "2) Run all cells sequentially, from top to bottom, so that cell numbers are sequential numbers (i.e. 1,2,3,4,5...)\n",
    "- Easiest way to do this is to click on the **Cell** tab at the top of your notebook and select **Run All** from the drop down menu. \n",
    "\n",
    "3) Comment out the cell that generates a pyLDAvis visual in objective 4 (see instructions in that section). \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bec125eb29f89460cf0c19ba9aa9a2f",
     "grade": false,
     "grade_id": "cell-395851cd95d17235",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nDuEqIyRc8YKS1q1fX0CZg</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-03-31 16:50:30</td>\n",
       "      <td>0</td>\n",
       "      <td>eZs2tpEJtXPwawvHnHZIgQ</td>\n",
       "      <td>1</td>\n",
       "      <td>BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...</td>\n",
       "      <td>10</td>\n",
       "      <td>n1LM36qNg4rqGXIcvVXv8w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eMYeEapscbKNqUDCx705hg</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-16 05:31:03</td>\n",
       "      <td>0</td>\n",
       "      <td>DoQDWJsNbU0KL1O29l_Xug</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here for lunch Togo. Service was quick. S...</td>\n",
       "      <td>0</td>\n",
       "      <td>5CgjjDAic2-FAvCtiHpytA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6Q7-wkCPc1KF75jZLOTcMw</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-06-20 19:14:48</td>\n",
       "      <td>1</td>\n",
       "      <td>DDOdGU7zh56yQHmUnL1idQ</td>\n",
       "      <td>3</td>\n",
       "      <td>I've been to Vegas dozens of times and had nev...</td>\n",
       "      <td>2</td>\n",
       "      <td>BdV-cf3LScmb8kZ7iiBcMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>k3zrItO4l9hwfLRwHBDc9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2010-07-13 00:33:45</td>\n",
       "      <td>4</td>\n",
       "      <td>LfTMUWnfGFMOfOIyJcwLVA</td>\n",
       "      <td>1</td>\n",
       "      <td>We went here on a night where they closed off ...</td>\n",
       "      <td>5</td>\n",
       "      <td>cZZnBqh4gAEy4CdNvJailQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6hpfRwGlOzbNv7k5eP9rsQ</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-06-30 02:30:01</td>\n",
       "      <td>0</td>\n",
       "      <td>zJSUdI7bJ8PNJAg4lnl_Gg</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5 to 4 stars\\n\\nNot bad for the price, $12.9...</td>\n",
       "      <td>5</td>\n",
       "      <td>n9QO4ClYAS7h9fpQwa5bhA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                date  funny  \\\n",
       "0  nDuEqIyRc8YKS1q1fX0CZg     1 2015-03-31 16:50:30      0   \n",
       "1  eMYeEapscbKNqUDCx705hg     0 2015-12-16 05:31:03      0   \n",
       "2  6Q7-wkCPc1KF75jZLOTcMw     1 2010-06-20 19:14:48      1   \n",
       "3  k3zrItO4l9hwfLRwHBDc9w     3 2010-07-13 00:33:45      4   \n",
       "4  6hpfRwGlOzbNv7k5eP9rsQ     1 2018-06-30 02:30:01      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  eZs2tpEJtXPwawvHnHZIgQ      1   \n",
       "1  DoQDWJsNbU0KL1O29l_Xug      4   \n",
       "2  DDOdGU7zh56yQHmUnL1idQ      3   \n",
       "3  LfTMUWnfGFMOfOIyJcwLVA      1   \n",
       "4  zJSUdI7bJ8PNJAg4lnl_Gg      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  BEWARE!!! FAKE, FAKE, FAKE....We also own a sm...      10   \n",
       "1  Came here for lunch Togo. Service was quick. S...       0   \n",
       "2  I've been to Vegas dozens of times and had nev...       2   \n",
       "3  We went here on a night where they closed off ...       5   \n",
       "4  3.5 to 4 stars\\n\\nNot bad for the price, $12.9...       5   \n",
       "\n",
       "                  user_id  \n",
       "0  n1LM36qNg4rqGXIcvVXv8w  \n",
       "1  5CgjjDAic2-FAvCtiHpytA  \n",
       "2  BdV-cf3LScmb8kZ7iiBcMA  \n",
       "3  cZZnBqh4gAEy4CdNvJailQ  \n",
       "4  n9QO4ClYAS7h9fpQwa5bhA  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load reviews from URL\n",
    "data_url = 'https://raw.githubusercontent.com/LambdaSchool/data-science-practice-datasets/main/unit_4/unit1_nlp/review_sample.json'\n",
    "\n",
    "# Import data into a DataFrame named df\n",
    "# YOUR CODE HERE\n",
    "\n",
    "df = pd.read_json(data_url, lines=True)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "356579363f311da83f4ef7abaf3c9212",
     "grade": true,
     "grade_id": "cell-cb5006475e42b8f9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert isinstance(df, pd.DataFrame), 'df is not a DataFrame. Did you import the data into df?'\n",
    "assert df.shape[0] == 10000, 'DataFrame df has the wrong number of rows.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenize Function\n",
    "<a id=\"#p1\"></a>\n",
    "\n",
    "Complete the function `tokenize`. Your function should\n",
    "- accept one document at a time\n",
    "- return a list of tokens\n",
    "\n",
    "You are free to use any method you have learned this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Consider using spaCy in your function. The spaCy library can be imported by running this cell.\n",
    "# A pre-trained model (en_core_web_sm) has been made available to you in the CodeGrade container.\n",
    "# If you DON'T need use the en_core_web_sm model, you can comment it out below.\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4837ed2a1cc13057ba40203859d46ff6",
     "grade": false,
     "grade_id": "cell-3d570d5a1cd6cb64",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "ENG_ALPHA_SET = set(string.ascii_letters)\n",
    "\n",
    "STOP_WORDS = ['\\n', ' ', '\\n\\n', ' \\n\\n', '  ','s','$','n','+']\n",
    "REINSERT_WORDS = [] #['not', 'nothing']\n",
    "def tokenize(doc):\n",
    "# YOUR CODE HERE\n",
    "    if (set(doc).intersection(ENG_ALPHA_SET)):\n",
    "        return [ token.lemma_ for token in nlp(doc.lower(), disable=['parser', 'tagger', 'ner']) if (token.is_stop == False and token.is_punct == False and token.text not in STOP_WORDS or token.text in REINSERT_WORDS )]\n",
    "    \n",
    "    return ['Not', 'an', 'english', 'language', 'review']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2181ca9d36070260b1f75dcfd9e58965",
     "grade": true,
     "grade_id": "cell-02da164f6fbe730a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''Testing'''\n",
    "assert isinstance(tokenize(df.sample(n=1)[\"text\"].iloc[0]), list), \"Make sure your tokenizer function accepts a single document and returns a list of tokens!\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize()\n",
    "\n",
    "# The object `Counter` takes an iterable, but you can instaniate an empty one and update it. \n",
    "word_counts = Counter()\n",
    "\n",
    "# Update it based on the token list for each job description\n",
    "df['clean_text'] = df.text.parallel_apply(lambda x: ' '.join(tokenize(x)))\n",
    "\n",
    "#df['clean_text'] = df.text.apply(lambda x: ' '.join(tokenize(x)))\n",
    "\n",
    "#df.clean_text.apply(lambda x: word_counts.update(x.split()))\n",
    "\n",
    "#display(word_counts.most_common(1000))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Rows 6204 and 6311 are not english language reviews - remove them\n",
    "display(df[df.clean_text == 'Not an english language review'])\n",
    "\n",
    "df.drop([6204, 6311], inplace=True)\n",
    "\n",
    "display(df[df.clean_text == 'Not an english language review'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.head()\n",
    "\n",
    "df['text_with_not'] = df.clean_text.apply(lambda x: 'not' in x.split())\n",
    "\n",
    "display(df[df.text_with_not == True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Representation\n",
    "<a id=\"#p2\"></a>\n",
    "1. Create a vector representation of the reviews (i.e. create a doc-term matrix).\n",
    "2. Write a fake review and query for the 10 most similiar reviews, print the text of the reviews. Do you notice any patterns?\n",
    "    - Given the size of the dataset, use `NearestNeighbors` model for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f570a35b1d17ce543ee41f516a0828c",
     "grade": false,
     "grade_id": "cell-0e96491cb529202c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.58 s, sys: 203 ms, total: 5.78 s\n",
      "Wall time: 5.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create a vector representation of the reviews \n",
    "# Name that doc-term matrix \"dtm\"\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from pandarallel import pandarallel\n",
    "\n",
    "\n",
    "#vect = TfidfVectorizer( ngram_range = (1,2), stop_words='english', min_df=3)\n",
    "#dtm = vect.fit_transform(df.clean_text)\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to clean text using tokenize()\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        #pandarallel.initialize()\n",
    "        return pd.Series(X).apply(lambda x: ' '.join(tokenize(x)))\n",
    "    \n",
    "cleaner = TextCleaner()\n",
    "vect = TfidfVectorizer(tokenizer=lambda x: x.split(), ngram_range = (1,2), stop_words='english', min_df=3, max_df=0.5)\n",
    "\n",
    "vectp = Pipeline(\n",
    "    [\n",
    "        ('cleaner', cleaner),\n",
    "        ('vect',vect)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dtm = vectp.fit_transform(df.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32b220e23c9aa1f602f08d1c2e879d0a",
     "grade": false,
     "grade_id": "cell-3d5bc610a8ec6b24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit a NearestNeighbors model named \"nn\"\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Fit on DTM\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
    "nn.fit(dtm.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d270ed23df3c7d3c6cf08ab174ccaf9e",
     "grade": true,
     "grade_id": "cell-c43704dcff67e99b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''Testing.'''\n",
    "assert nn.__module__ == 'sklearn.neighbors._unsupervised', ' nn is not a NearestNeighbors instance.'\n",
    "assert nn.n_neighbors == 10, 'nn has the wrong value for n_neighbors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3da2ced9f187ed0aa1a890785e2ba00e",
     "grade": false,
     "grade_id": "cell-496203e8746296ca",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review[0:6343] = 1:This mall is not that great when this mall was under construction the developer advertised this mall being better than West Edmonton mall which is completely unture. Comparing the largest indoor water park and an amazing amusement park at West Edmonton mall the best you can do at Cross Iron Mills mall is eat at the food court or buy some fishing gear because this mall actually has a store which has a lots of fishing equipment. I have been to quite a few malls in different cities of Canada and the United States but nothing special at this mall. \n",
      "\n",
      "\n",
      "review[1:5995] = 4:I like this mall. Parking is always available and free. I never have to fight for a parking spot in the big garage.  This mall has everything from a huge gym, 2 foodcourts, and plenty of stores galore.  I always like the fact that this mall is always clean no matter what the season. \n",
      "\n",
      "\n",
      "review[2:21] = 5:Yorkdale Shopping Centre is the best mall in Toronto.\n",
      "\n",
      "It has the most free parking (eventho it's hard to park on a nice weekend sometimes). The store selection is great from very thifty to most expensive stores. \n",
      "\n",
      "It is the first place to have a Bath & Body Works in Toronto.\n",
      "\n",
      "I have never had a bad experience at this shopping mall and whenever friends come visit from out of town, I take them here for shopping and not the silly Eaton's Centre. \n",
      "\n",
      "The movie theatre here is also good and not usually too busy.\n",
      "\n",
      "The restaurant selection is ok too with places like Rainforest Cafe you can't go wrong for some novelty action.\n",
      "\n",
      "My favorite stores here are RW&Co (much larger than anywhere else) and Home Outfitters (the second floor). The also have a Marciano Outlet with crazy deals. \n",
      "\n",
      "\n",
      "review[3:6274] = 5:This review will be for the secondary store inside the mall. Granted, I do love the store outside the mall just as much!!\n",
      "\n",
      "Overall: So this Starbucks has just opened inside the mall, near the food court a couple months ago. It is always packed, but the staff there are amazing! As far as the food/drinks go, they are always standard fare. \n",
      "\n",
      "Staff: This is where they truly shine! I have read reviews on here about a couple of the staff being nasty towards others, and I don't see it! If you have a negative attitude towards others, of course they won't be friendly back. Don't believe you're entitled to everything in life, and you'd be amazed about how less stressed your life can become! Colton is one of the baristas there, and he is awesome. Always recommending drinks for me. The manager is super nice as well. I have spoken with her a couple times, and she is the one wearing the black apron. If you're not aware, that black apron took a long time to achieve, so congrats to her!\n",
      "\n",
      "Wifi: It's present, but everyone in the mall is on it, so beware. \n",
      "\n",
      "Seating: Seating is somewhat limited, and usually dirty because others didn't learn how to clean up after themselves apparently. \n",
      "\n",
      "\n",
      "review[4:9083] = 5:Shops at Don Mills has officially become my favorite shopping \"mall\" in Toronto. At first I was skeptical about the concept and how it would survive in the winter. I have been shopping here year round and I absolutely love it. \n",
      "\n",
      "I love how much work they have put into keeping SADM busy and active. Between the free concerts, movie screenings in the centre court, ice rink in the winter, and various other events there is always something going on here. \n",
      "\n",
      "I really appreciate the collection of \"regular\" stores along with specialty shops. The restaurants are great, and they finally opened a fast food option (besides Pizza Pizza), South Street Burger Company. \n",
      "\n",
      "My ultimate pampering Saturday is spent getting my eyebrows done at Murale, hair done at Donato and then a pedicure at Aphrodite. Top it off with a Teaopia Tea and I'm set. \n",
      "\n",
      "\n",
      "review[5:8367] = 4:San Tan Village is a great mall. The parking can be a little hectic especially if you get caught up in the heart of the pedestrian area but if you just park on the outskirts by one of the anchor stores it's not bad at all. The mall is layed out nicely with wide open walkways and misters when it's hot. Recently I have taken my baby niece to the splash pad and play area which are both great for the little ones. The mall is kept very clean and there are plenty of spots to sit down and relax. Lots of great stores to choose from including higher end and also your average mall stores. It of course gets a quite hot in the summer and is much more enjoyable in the fall/winter months but would not be bad to stop by if you had a specific store in mind in the summer. There are also multiple great restaurants to try including, Kona Grill, Blue Wasabi, and Cantina Laredo to name just a few.\n",
      "\n",
      "Overall San Tan Village is nice outdoor venue for walking around and enjoying the day. I will be back! \n",
      "\n",
      "\n",
      "review[6:7940] = 4:Other than the weirdness of the levels, this is how I think a mall should be. Vegas is a shop til you drop town, but most malls either cater to the budget conscious or the money is no object crowd.   Whether you are shopping for food, clothes, jewelry or some other things there are several price ranges represented.  Because of that, and parking is off of Spring Mountain instead of the strip it fronts, locals shop here as well as tourists.  Even to tourists it draws from all the three other corners.(Wynn/Encore, Venetian/Palazzo, Treasure Island).  Bonus points: Sur la Table is there:-) \n",
      "\n",
      "\n",
      "review[7:6578] = 4:This mall was the fancy malls in the area when I was growing up, and meant a special occasion when we were able to go. It wasn't for commoners like my family. Ours was on the other side of town (and is now torn down). All one level, several corridors off one main corridor, a food court that was better than the **other** mall.\n",
      "\n",
      "I came this last trip to visit my parents. The higher end stores are still here, and it is just as nice as ever. American Doll, Neiman Marcus, Belks and Nordstrom are the highlights now. Everything is so shiny and clean. The style here is Southern Genteel, for the upper crust of society. As mentioned in other reviews, some of the sales people at the kiosks can be a bit aggressive, but there are ones that are content to stare at their phones and look pretty.\n",
      "\n",
      "Parking has been greatly improved with the installation of several parking garages, which is nice. I am sure that things can still get hairy during the holidays and back to school shopping. I am glad I got to visit, and glad to have gotten that out of my system. \n",
      "\n",
      "\n",
      "review[8:2945] = 4:While in Tempe for a wedding a while back, I decided to go on an adventure in my rental car!  Boy, am I glad I did since it is impossible to be outside in the middle of June!  SFM had Neiman Marcus, Nordstrom, Dillard's, Macy's,  and Robinson's May as their anchor stores.  They had smaller stores like Eddie Bauer, J. Crew, and Louis Vuitton.  This facility is the largest shopping mall in the Southwest.  It was very clean and bright. \n",
      "\n",
      "\n",
      "review[9:7432] = 4:Great BBQ in the Santan Village mall. A really nice shopping mall with a uniquely different layout. \n",
      "This place was very nicely decorated with different types of BBQ themed signs and banners. They even have a small sports bar type area. Outside seating was available weather permitting. \n",
      "We had the Sassy Barbecue salad and The feast for one. The salad was really big with a lot of brisket and a really tasty dressing.  \n",
      "There was so much food with the individual feast, we had leftovers for lunch or dinner the next day. Bab back ribs barbeque  chicken pull pork coleslaw baked beans corn on the cob and corn bread and potato wedges. The meats were so tender and full of flavor. You could also try some of the other BBQ sauces that were in the table. \n",
      "I had one oh the specialty cocktails. Mamma's pounce was really refreshing and tasty. It was like a tropical drink. \n",
      "No room for dessert. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a fake review and find the 10 most similar reviews\n",
    "\n",
    "# YOUR CODE HERE\n",
    "fake_review = \"As far as shopping malls go the Great Mall in Milpitas is one of the best i've been to \"\n",
    "\n",
    "neighs = nn.kneighbors(vectp.transform([fake_review]).todense())[1][0]\n",
    "\n",
    "for idx,neigh in enumerate(neighs):\n",
    "    print(f'review[{idx}:{neigh}] = {df.iloc[neigh].stars}:{df.iloc[neigh].text} \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Classification\n",
    "<a id=\"#p3\"></a>\n",
    "Your goal in this section will be to predict `stars` from the review dataset. \n",
    "\n",
    "1. Create a pipeline object with a sklearn `CountVectorizer` or `TfidfVector` and any sklearn classifier.\n",
    "    - Use that pipeline to train a model to predict the `stars` feature (i.e. the labels). \n",
    "    - Use that Pipeline to predict a star rating for your fake review from Part 2. \n",
    "\n",
    "\n",
    "\n",
    "2. Create a parameter dict including `one parameter for the vectorizer` and `one parameter for the model`. \n",
    "    - Include 2 possible values for each parameter\n",
    "    - **Use `n_jobs` = 1** \n",
    "    - Due to limited computational resources on CodeGrader `DO NOT INCLUDE ADDITIONAL PARAMETERS OR VALUES PLEASE.`\n",
    "    \n",
    "    \n",
    "3. Train the entire pipeline with a GridSearch\n",
    "    - Name your GridSearch object as `gs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1d18da8521d51d8bfc4b5b9d005fa34",
     "grade": false,
     "grade_id": "cell-e2beb0252d274bba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   33.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "CPU times: user 34.8 s, sys: 416 ms, total: 35.2 s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Name the gridsearch instance \"gs\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "vect = TfidfVectorizer(ngram_range = (1,2), stop_words='english', min_df=3)\n",
    "\n",
    "#clf = KNeighborsClassifier()\n",
    "clf = LinearSVC()\n",
    "# Define the Pipeline\n",
    "pipe = Pipeline([\n",
    "    ('vect', vect),      # TF-IDF Vectorizer\n",
    "    ('clf', clf)         # \n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.7),\n",
    "#   'clf__n_neighbors': (20, 25)\n",
    "    'clf__C': ( 0.5, 0.7)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, parameters, n_jobs=1, verbose=1)\n",
    "gs.fit(df.text, df.stars)\n",
    "\n",
    "print(gs.predict([fake_review])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    4462\n",
       "4    2185\n",
       "1    1496\n",
       "3    1098\n",
       "2     759\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stars.value_counts()\n",
    "\n",
    "# baseline accuracy is 0.4462\n",
    "# The best 'max_df' value is 0.5 for both KNeighborsClassifier and LinearSVC\n",
    "# KNeighborsClassifier goes from 0.5136 to 0.52.. to 0.53.. as n_neighbors increases from 10 to 25 \n",
    "# LinearSVC gets 0.6159 with a best 'C' value of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9e2378efb868f104a4eb39e4f25563c",
     "grade": true,
     "grade_id": "cell-d07134c6fe5d056e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "prediction = gs.predict([\"I wish dogs knew how to speak English.\"])[0]\n",
    "assert prediction in df.stars.values, 'You gs object should be able to accept raw text within a list. Did you include a vectorizer in your pipeline?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{'clf__C': 0.5, 'vect__max_df': 0.5}\n",
      "0.6159000000000001\n",
      "Pipeline(memory=None,\n",
      "         steps=[('vect',\n",
      "                 TfidfVectorizer(analyzer='word', binary=False,\n",
      "                                 decode_error='strict',\n",
      "                                 dtype=<class 'numpy.float64'>,\n",
      "                                 encoding='utf-8', input='content',\n",
      "                                 lowercase=True, max_df=0.5, max_features=None,\n",
      "                                 min_df=3, ngram_range=(1, 2), norm='l2',\n",
      "                                 preprocessor=None, smooth_idf=True,\n",
      "                                 stop_words='english', strip_accents=None,\n",
      "                                 sublinear_tf=False,\n",
      "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                                 tokenizer=None, use_idf=True,\n",
      "                                 vocabulary=None)),\n",
      "                ('clf',\n",
      "                 LinearSVC(C=0.5, class_weight=None, dual=True,\n",
      "                           fit_intercept=True, intercept_scaling=1,\n",
      "                           loss='squared_hinge', max_iter=1000,\n",
      "                           multi_class='ovr', penalty='l2', random_state=None,\n",
      "                           tol=0.0001, verbose=0))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Topic Modeling\n",
    "\n",
    "Let's find out what those yelp reviews are saying! :D\n",
    "\n",
    "1. Estimate a LDA topic model of the review text\n",
    "    - Set num_topics to `5`\n",
    "    - Name your LDA model `lda`\n",
    "2. Create 1-2 visualizations of the results\n",
    "    - You can use the most important 3 words of a topic in relevant visualizations. Refer to yesterday's notebook to extract. \n",
    "3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model\n",
    "\n",
    "When you instantiate your LDA model, it should look like this: \n",
    "\n",
    "```python\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )\n",
    "\n",
    "```\n",
    "\n",
    "__*Note*__: You can pass the DataFrame column of text reviews to gensim. You do not have to use a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about  pyLDAvis\n",
    "\n",
    "**pyLDAvis** is the Topic modeling package that we used in class to visualize the topics that LDA generates for us.\n",
    "\n",
    "You are welcomed to use pyLDAvis if you'd like for your visualization. However, **you MUST comment out the code that imports the package and the cell that generates the visualization before you submit your notebook to CodeGrade.** \n",
    "\n",
    "Although you should leave the print out of the visualization for graders to see (i.e. comment out the cell after you run it to create the viz). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "# Due to limited computationalresources on CodeGrader, use the non-multicore version of LDA \n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estimate a LDA topic model of the review tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9514841e71735eaa255bccc53b257896",
     "grade": false,
     "grade_id": "cell-66331a185ff52f15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.013*\"place\" + 0.012*\"food\" + 0.010*\"good\"'), (1, '0.015*\"place\" + 0.014*\"great\" + 0.012*\"love\"'), (2, '0.018*\"good\" + 0.016*\"order\" + 0.014*\"food\"'), (3, '0.013*\"come\" + 0.011*\"food\" + 0.010*\"service\"'), (4, '0.014*\"time\" + 0.010*\"service\" + 0.009*\"great\"')]\n",
      "CPU times: user 8.5 s, sys: 18.2 ms, total: 8.52 s\n",
      "Wall time: 8.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Remember to read the LDA docs for more information on the various class attirbutes and methods available to you\n",
    "# in the LDA model: https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "\n",
    "# don't change this value \n",
    "num_topics = 5\n",
    "\n",
    "# use tokenize function you created earlier to create tokens \n",
    "df['tokens'] = df.text.apply(tokenize)\n",
    "\n",
    "# create a id2word object (hint: use corpora.Dictionary)\n",
    "\n",
    "id2word = corpora.Dictionary(df.tokens)\n",
    "# create a corpus object (hint: id2word.doc2bow)\n",
    "corpus = [id2word.doc2bow(text) for text in df.tokens]\n",
    "# How many words are in our vocab?\n",
    "len(id2word.keys())\n",
    "# instantiate an lda model\n",
    "lda = LdaModel(corpus=corpus,\n",
    "               id2word=id2word,\n",
    "               random_state=723812,\n",
    "               num_topics = num_topics,\n",
    "               passes=1\n",
    "              )\n",
    "\n",
    "# YOUR CODE HERE\n",
    "topic_words = lda.print_topics(num_words=3)\n",
    "\n",
    "print(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6479db0fa59c99d3ae3201c1f10ebca1",
     "grade": true,
     "grade_id": "cell-5a3c181311134fa9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible Testing\n",
    "assert lda.get_topics().shape[0] == 5, 'Did your model complete its training? Did you set num_topics to 5?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create 1-2 visualizations of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "189591ed7b9e6e6146d59761fb418268",
     "grade": false,
     "grade_id": "cell-9b043e992fbd218c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/U4-S1-NLP/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el17941405764580758569226187835\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el17941405764580758569226187835_data = {\"mdsDat\": {\"x\": [0.020068652702419906, 0.07383317087441588, 0.06947021385476279, -0.07255643382459785, -0.09081560360700075], \"y\": [-0.01716158101633525, 0.06633142731322877, -0.0528374014708432, -0.03008641618832141, 0.03375397136227103], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [26.495003490008965, 11.097117077579599, 23.424227753957265, 12.588054564857451, 26.39559711359672]}, \"tinfo\": {\"Term\": [\"love\", \"food\", \"great\", \"order\", \"chicken\", \"place\", \"pizza\", \"come\", \"good\", \"wait\", \"friendly\", \"sushi\", \"nice\", \"minute\", \"selection\", \"store\", \"restaurant\", \"tell\", \"nail\", \"taste\", \"delicious\", \"dr\", \"beer\", \"staff\", \"car\", \"try\", \"shop\", \"room\", \"call\", \"amaze\", \"brake\", \"nigiri\", \"yellowtail\", \"alex\", \"bisque\", \"uptown\", \"sushi\", \"knot\", \"jalape\\u00f1os\", \"keg\", \"ikea\", \"installer\", \"port\", \"2011\", \"cave\", \"japanese\", \"bare\", \"froyo\", \"wi\", \"scott\", \"cox\", \"verify\", \"seaweed\", \"lawyer\", \"hoagie\", \"250\", \"fi\", \"baba\", \"tucson\", \"exhaust\", \"sashimi\", \"sucker\", \"decently\", \"dim\", \"ayce\", \"uni\", \"poutine\", \"library\", \"fish\", \"dragon\", \"mayo\", \"roll\", \"tuna\", \"connect\", \"sum\", \"pig\", \"game\", \"salmon\", \"room\", \"lobster\", \"crab\", \"eat\", \"steak\", \"price\", \"lunch\", \"beer\", \"place\", \"worth\", \"hotel\", \"small\", \"food\", \"night\", \"like\", \"menu\", \"try\", \"great\", \"come\", \"get\", \"good\", \"bar\", \"open\", \"sandwich\", \"large\", \"fry\", \"restaurant\", \"fresh\", \"go\", \"bite\", \"nice\", \"service\", \"time\", \"well\", \"want\", \"think\", \"order\", \"little\", \"love\", \"look\", \"drink\", \"enchiladas\", \"heartbeat\", \"festival\", \"izakaya\", \"bouchon\", \"jamba\", \"chorizo\", \"arepa\", \"magician\", \"und\", \"personalize\", \"mince\", \"frog\", \"aunt\", \"asado\", \"cowboy\", \"musician\", \"teller\", \"domino\", \"britney\", \"bridal\", \"ragu\", \"halo\", \"oxtail\", \"brussel\", \"gyoza\", \"drywall\", \"enthusiastic\", \"mex\", \"chang\", \"trail\", \"frill\", \"pho\", \"comic\", \"collection\", \"audience\", \"brewery\", \"taiwanese\", \"noir\", \"ipa\", \"kim\", \"boba\", \"selection\", \"nail\", \"art\", \"cupcake\", \"greek\", \"tart\", \"accessory\", \"mint\", \"mein\", \"study\", \"love\", \"korean\", \"shop\", \"friendly\", \"bike\", \"beer\", \"nice\", \"great\", \"store\", \"awesome\", \"chocolate\", \"wonderful\", \"place\", \"helpful\", \"beef\", \"try\", \"amaze\", \"staff\", \"super\", \"local\", \"food\", \"delicious\", \"fresh\", \"well\", \"find\", \"bar\", \"chicken\", \"good\", \"bite\", \"service\", \"like\", \"come\", \"new\", \"recommend\", \"definitely\", \"time\", \"enchilada\", \"iphone\", \"concoction\", \"yuck\", \"penne\", \"urban\", \"earl\", \"pizzeria\", \"pecan\", \"contractor\", \"mary\", \"poach\", \"skinny\", \"yolk\", \"tamale\", \"alfredo\", \"unorganized\", \"pepperoni\", \"gabi\", \"horchata\", \"mocha\", \"quinoa\", \"ranchero\", \"pear\", \"umami\", \"prickly\", \"carl\", \"rancheros\", \"sever\", \"category\", \"pancake\", \"crust\", \"omelette\", \"benedict\", \"biscuit\", \"robert\", \"thai\", \"gluten\", \"slider\", \"church\", \"pizza\", \"egg\", \"syrup\", \"queso\", \"muffin\", \"bacon\", \"crispy\", \"brunch\", \"chicken\", \"order\", \"breakfast\", \"taste\", \"cream\", \"ice\", \"cheese\", \"burger\", \"good\", \"wing\", \"toast\", \"pad\", \"thin\", \"sauce\", \"salsa\", \"fry\", \"onion\", \"tomato\", \"potato\", \"food\", \"flavor\", \"dish\", \"like\", \"pretty\", \"sweet\", \"meal\", \"salad\", \"delicious\", \"well\", \"drink\", \"little\", \"time\", \"place\", \"seat\", \"restaurant\", \"come\", \"service\", \"great\", \"go\", \"try\", \"get\", \"eat\", \"price\", \"think\", \"menu\", \"\\u00e0\", \"une\", \"vous\", \"je\", \"en\", \"lens\", \"ipad\", \"mais\", \"dental\", \"dans\", \"et\", \"des\", \"j'ai\", \"avec\", \"pas\", \"les\", \"que\", \"cesar\", \"hakka\", \"est\", \"sur\", \"tr\\u00e8s\", \"possibility\", \"ou\", \"capacity\", \"comme\", \"sont\", \"un\", \"del\", \"x\", \"canal\", \"exhibit\", \"dr\", \"le\", \"tan\", \"de\", \"doctor\", \"exam\", \"dentist\", \"pedicure\", \"la\", \"massage\", \"insurance\", \"therapist\", \"minute\", \"tooth\", \"wait\", \"patient\", \"come\", \"restaurant\", \"table\", \"service\", \"sit\", \"nail\", \"food\", \"ask\", \"go\", \"experience\", \"tell\", \"recommend\", \"time\", \"leave\", \"order\", \"get\", \"say\", \"take\", \"2\", \"well\", \"staff\", \"place\", \"room\", \"look\", \"like\", \"feel\", \"give\", \"check\", \"good\", \"want\", \"great\", \"yuk\", \"cajun\", \"hall\", \"teacher\", \"removal\", \"dolphin\", \"eyebrow\", \"vet\", \"laser\", \"apartment\", \"professionally\", \"wire\", \"thread\", \"depot\", \"humor\", \"curly\", \"harness\", \"instruct\", \"ford\", \"drama\", \"stylist\", \"horribly\", \"storage\", \"a.m.\", \"tenant\", \"accent\", \"severe\", \"tim\", \"arrogant\", \"vaccination\", \"hair\", \"instructor\", \"cat\", \"maintenance\", \"tone\", \"car\", \"driver\", \"pet\", \"haircut\", \"yoga\", \"rend\", \"wax\", \"fix\", \"concern\", \"repair\", \"call\", \"company\", \"class\", \"tire\", \"job\", \"phone\", \"dog\", \"appointment\", \"customer\", \"tell\", \"work\", \"need\", \"month\", \"email\", \"time\", \"help\", \"say\", \"professional\", \"day\", \"thank\", \"review\", \"people\", \"know\", \"store\", \"care\", \"service\", \"go\", \"year\", \"experience\", \"take\", \"look\", \"great\", \"get\", \"want\", \"place\", \"ask\", \"come\", \"find\", \"staff\", \"like\", \"well\", \"love\", \"good\", \"order\", \"food\"], \"Freq\": [2269.0, 4966.0, 4294.0, 3619.0, 1474.0, 5284.0, 928.0, 3846.0, 4818.0, 1729.0, 1422.0, 522.0, 1757.0, 1041.0, 521.0, 824.0, 1668.0, 1525.0, 383.0, 1303.0, 1266.0, 263.0, 609.0, 1616.0, 703.0, 2535.0, 637.0, 1095.0, 818.0, 1415.0, 40.15521241510018, 35.26122617533595, 29.274152711492444, 20.033794633826897, 26.6762747111244, 15.7136675517972, 496.90956000490917, 15.424214711575186, 15.533747429592788, 26.416180619793625, 18.79264517664035, 16.305527708062527, 18.078721602013886, 15.07336266937548, 13.635019509628718, 99.35021813569684, 16.193459904597606, 12.706748784866768, 22.43134412821218, 12.347167724567106, 23.83491121281516, 11.858593381732025, 37.65663976563913, 12.736917247617303, 13.60710046224898, 12.389010823359001, 21.433243150505387, 10.700347840164737, 11.014324794307374, 11.500741476730013, 86.14711475508297, 14.530311421317748, 16.295480067159, 52.6156871080337, 65.28440582748102, 33.674459438212935, 52.40618310621549, 32.837079094156714, 307.8512417745016, 26.9399462999036, 39.749046321923764, 417.87497935728226, 104.29063805995291, 36.524474388574866, 45.46381755533831, 39.987219491554455, 179.7318372461611, 156.93919425348045, 635.5247025999934, 115.97953868659113, 111.4866180994317, 799.9729561619873, 283.2902745221543, 871.8104065224101, 382.76763286859875, 312.5321748928214, 1754.0870987158594, 315.05926914541965, 256.5616351617042, 418.7727174372425, 1608.7732766044492, 469.87032348893706, 1269.1619092220383, 549.0597335343318, 891.3092848838684, 1303.6745473670378, 1175.0132853510256, 953.7334105567807, 1347.8174227048078, 439.68773236667096, 319.9375302879912, 285.5543555514477, 273.6106069468708, 448.67516269903143, 556.0745495570676, 383.567660431258, 782.8999502532962, 421.0452437064141, 546.3779006929967, 834.483535968952, 861.4557554200838, 693.8102974964434, 517.1323679610916, 480.5703516056387, 692.5696645156203, 464.6570375615182, 538.2169536209786, 470.90866421848307, 420.9862178362974, 45.74560652150536, 16.66446890956896, 14.471415348916574, 20.27693336820293, 15.368266900291461, 10.096548942858606, 35.02614477760771, 10.182022276635397, 8.843849436595685, 8.245043239247146, 8.5160113536419, 10.088105332138886, 10.742364721022536, 7.993924266043176, 9.909478116658669, 8.868345891682035, 9.314609259931967, 7.639953461473742, 7.926068728882588, 7.918533186658063, 11.265941484686678, 12.224686699436944, 8.32577734294561, 19.055188221246116, 16.81062039444222, 6.87829080541625, 7.221903607045727, 11.50130792587976, 12.070747695217454, 10.245247139899021, 34.28051753353736, 14.024831066407868, 86.0254774561516, 11.308747484902586, 31.19973114074456, 18.816805186020567, 32.026247795167, 20.65650758395626, 17.71024686367939, 26.068892165770457, 17.961574233056343, 46.310846672148, 273.9220152515321, 200.62214312705046, 61.49208474242647, 43.57555102368416, 36.651245075464274, 29.167724240857822, 18.419806514587183, 34.224144387793764, 15.329996488266213, 24.458165657183386, 657.7534716379375, 51.41142779061467, 224.74034093467932, 402.69889471103244, 45.09575277034709, 207.0892769993648, 430.58509817566596, 773.4875335104934, 243.3979568007307, 207.96543473152457, 120.10446717422651, 142.21891755388194, 822.7798622332948, 142.07090080440335, 150.60035489977386, 450.8401285514219, 303.6863262308772, 324.65924101112785, 185.65027744300446, 121.17781998316485, 569.1872977703187, 247.20714090761172, 205.28579274695997, 395.5863570986148, 276.58522642604754, 219.67103033549222, 249.45587982739877, 411.64311265729725, 200.28231949081717, 332.9297957453761, 317.01031859040705, 304.086675088696, 194.2928454945932, 205.18708110572314, 196.90824757597937, 200.0752735348656, 31.96527692310562, 26.511633207648465, 19.582629288407446, 19.250852962189736, 18.825157446868435, 23.700826500175197, 23.45522862351179, 19.146117143296774, 23.811311005721333, 29.758048862445758, 52.217697619030346, 36.391176894899885, 19.034508658282807, 17.16682275221735, 27.78181530830998, 21.435168233073096, 18.86651554255451, 37.928536608971754, 14.564050929013835, 13.889471736933066, 45.53784749959054, 14.197636967678118, 10.825321395481625, 35.691292275202244, 10.525335122471233, 11.554937948244971, 17.583370881066084, 9.65523953470252, 14.792524844677766, 20.043363860085265, 159.43291486271676, 152.52210034334576, 44.351677846383865, 48.010077136030326, 56.851711086753575, 41.404265818856864, 222.82629349790673, 67.50842803541242, 79.51440205095393, 42.84929195771726, 744.2894089190584, 390.76032001989495, 55.43303825220142, 42.63479920927296, 37.72927110617497, 171.4182290651829, 162.98838305645506, 151.37103832262383, 851.4292727059224, 1842.3404366800416, 337.170450871617, 739.6962935851855, 386.99247322956853, 380.3501292345136, 523.0213122426792, 357.0622113106246, 2077.3998684723633, 164.73081884575114, 137.06980456881703, 95.08225361934198, 102.68725926717774, 496.42271279629585, 154.58520479867227, 584.9873172379723, 163.1571673065409, 157.36378908881258, 224.8021022145651, 1605.0397898288663, 387.46499955012763, 430.2657897867018, 1215.9219517098782, 494.5091645137306, 340.4009551920728, 421.41627125381876, 369.46040063760717, 513.8904241830161, 889.0227998751179, 566.2485456416315, 539.9732240993288, 1005.0862042712462, 1089.8545948217416, 350.51481681918546, 517.6417324183158, 788.9566205847967, 734.4378359291887, 771.292855065188, 606.0305143575246, 552.9222704112118, 558.3894671112342, 456.4537888289203, 441.98126863264093, 426.6289133657935, 411.9283056217922, 47.73332058966597, 26.568133204766454, 22.103348144255307, 18.50036737657718, 37.68270990479518, 17.65667471176575, 16.827915376381146, 20.49221142034317, 50.340877292876684, 14.523258798579594, 58.09352416333295, 28.753398649721785, 15.952923301596131, 14.673899905193878, 21.63810850832611, 39.01077132055097, 25.394963510459398, 12.13712955640421, 11.81442708474475, 26.770469302070836, 11.598003466547942, 21.105615858410253, 14.098946048938581, 11.36034295330071, 11.06315765324943, 10.79528442770991, 15.882777820166622, 49.446499034807815, 16.20643935421313, 35.229239562303306, 30.60846080238043, 23.50684561427113, 227.140891393371, 61.633861169058896, 41.31775589421237, 113.90994119793938, 114.2905963850674, 20.95051740373845, 53.928263184814135, 106.9912741261704, 122.47124933913693, 112.84211915875103, 69.55920039031076, 30.577842713202898, 362.14799755509733, 53.826607964827566, 511.5298723331491, 90.82581468981138, 800.2812047179249, 380.53288734862514, 288.36976397610584, 628.7695127523434, 228.61076101407517, 139.51134490960123, 680.6396334475915, 318.5559209364695, 496.17653290423743, 286.5174630602354, 292.7940131457724, 280.1963017428389, 518.3425337988459, 227.3320063708629, 417.80708161854255, 372.5439390111533, 266.9395463969041, 259.170864219058, 251.23972927683923, 344.97514689717514, 260.9980263750025, 443.97575410360196, 210.6737812193423, 263.33636249136333, 319.6165522548552, 214.241558015651, 201.79641932670737, 198.44226446400273, 309.81410413096387, 233.76607638630205, 264.224790427968, 57.670937582298116, 34.252578293834844, 30.69774042038128, 53.76474523512322, 24.246851248971897, 24.197600669616723, 64.62438207039735, 84.83560153097987, 27.98209427166271, 86.16329959538457, 16.02205726984755, 14.912941987966947, 20.537389814241724, 28.51282858693429, 14.198946174561682, 20.185068460293483, 27.961361947109232, 14.463450113593336, 43.38426634590195, 17.5144416119644, 74.34364304787778, 17.874799568812282, 14.176375269771667, 12.67802284531858, 27.219549950801923, 11.889320927520027, 13.534398839973239, 22.89880911990991, 12.544128781460278, 11.16705668613148, 387.7724830025655, 48.564475114942255, 69.5386064239572, 59.58055807129693, 36.72885467752929, 577.1599307001774, 82.46708800080084, 90.52848676060569, 78.26851333471859, 33.55766093091072, 69.22011268465313, 62.48135330206193, 299.4681298639484, 114.49613956432945, 164.16789420805094, 563.4663434180122, 298.0692181494593, 175.68534131481204, 143.51560396637709, 366.3909577465611, 277.39244114773095, 288.74540176099833, 278.812733543105, 711.5045494356746, 827.875886291217, 820.5689405914268, 753.8885979767915, 297.29137148832126, 105.7556930936678, 1806.1957726193361, 369.637865942449, 746.5849694489298, 243.03778819633547, 746.7966050179717, 370.55693365941823, 497.8234915581956, 656.6113852397547, 720.6787010579408, 422.79195752066056, 361.41761234240187, 1246.8217940534755, 1068.1513146421014, 467.6846324108024, 608.6598556459656, 598.2848274733452, 728.2162203628741, 1181.6873741819813, 880.4657269243811, 654.2024549852371, 1173.8927438964763, 553.7834582463393, 777.90685273048, 530.7118640488438, 524.1875296753254, 692.7335178630103, 632.7411366662967, 568.1337249459248, 671.375695308241, 545.2612816885766, 502.8730716044948], \"Total\": [2269.0, 4966.0, 4294.0, 3619.0, 1474.0, 5284.0, 928.0, 3846.0, 4818.0, 1729.0, 1422.0, 522.0, 1757.0, 1041.0, 521.0, 824.0, 1668.0, 1525.0, 383.0, 1303.0, 1266.0, 263.0, 609.0, 1616.0, 703.0, 2535.0, 637.0, 1095.0, 818.0, 1415.0, 41.151325317424636, 36.415272461013544, 30.372901665498553, 20.854505669138522, 27.95282394060561, 16.526670274198004, 522.7474083118378, 16.2428856182345, 16.36569056272423, 27.844742371356535, 19.839555980023572, 17.21738911312054, 19.14194501898617, 15.96385172143535, 14.490876239036051, 105.61791846988966, 17.217178991969565, 13.535357026991631, 23.977532055549798, 13.20594916084614, 25.53112256096024, 12.705197682294848, 40.34898129417259, 13.663080811897771, 14.599842781340769, 13.305744016477627, 23.087351484333876, 11.53113865757299, 11.877649504523491, 12.402253943758806, 93.02011596342034, 15.674468705591078, 17.63107586893445, 57.65620308096616, 72.24705665503792, 36.812781935958945, 58.55307793328147, 36.39480810765019, 385.6616252276613, 29.908536146762366, 45.221794270696165, 554.4820730999506, 127.60146781167646, 41.499371533115244, 52.816060199704225, 46.009898421268815, 242.8190373779982, 218.38217244421355, 1095.8563582998133, 161.18365524437542, 154.36211224161116, 1552.5747705731894, 467.34822740147524, 1860.1850666748837, 757.2723015199315, 609.1784460114665, 5284.590053770975, 623.2794582198386, 482.3322133591265, 897.3972044553963, 4966.51306925572, 1040.7255940902228, 3814.444249640189, 1314.3511462254476, 2535.0882009016823, 4294.3671005526685, 3846.2446384729233, 2911.908158374891, 4818.050203273673, 1074.0418308785875, 682.990676156702, 584.3845238403819, 550.7160437921079, 1192.506485215913, 1668.7336684970574, 941.3307683514299, 3085.300789757747, 1122.2450355791914, 1757.5365044592318, 3777.442474449336, 4391.155539644378, 2956.135738033648, 1774.5267834746537, 1523.5176007166904, 3619.6431233022186, 1447.2491439912058, 2269.8404902658767, 2008.2480424916394, 1525.5096348171346, 47.78497493616386, 17.541217777546628, 15.338783482662105, 21.739041350565476, 16.607890052121327, 10.942710372152936, 38.07652083200019, 11.093679933526587, 9.691371120171588, 9.04338829051041, 9.341356073674898, 11.099387975107543, 11.82013425656479, 8.807221188309011, 10.930185690831738, 9.8130135447744, 10.31740775453648, 8.492524500073385, 8.819252598936943, 8.811942609546364, 12.563361181611127, 13.63610213478347, 9.289143764948388, 21.307127065026112, 18.82658979158042, 7.7047131588850455, 8.091789551378998, 12.944895681937023, 13.624880258879173, 11.569788436258081, 39.0536990454034, 15.841430881091586, 110.29964425082416, 12.85243780814543, 37.95335630098264, 22.167221395380377, 39.35438509916845, 25.016441591478102, 21.352992892644114, 33.464001884300316, 22.020726244355096, 65.61895619712692, 521.7488820387853, 383.4716041892367, 96.73120907723262, 64.84404819304815, 53.17484554188995, 40.60157070851267, 23.051559400027116, 49.86235250596492, 18.432568122970554, 33.08664243430938, 2269.8404902658767, 87.93746552406017, 637.4418064369979, 1422.1512221701037, 75.66712025764124, 609.1784460114665, 1757.5365044592318, 4294.3671005526685, 824.3492107764725, 676.4179650130706, 314.4975430271172, 402.75857253458594, 5284.590053770975, 415.24370993931313, 469.2049069340066, 2535.0882009016823, 1415.2503426761805, 1616.7674815142532, 705.1300687206622, 361.9930501185541, 4966.51306925572, 1266.8436947425107, 941.3307683514299, 2956.135738033648, 1593.7893840406025, 1074.0418308785875, 1474.2578338702683, 4818.050203273673, 1122.2450355791914, 3777.442474449336, 3814.444249640189, 3846.2446384729233, 1138.0355196021592, 1418.5901242369794, 1421.6794868166098, 4391.155539644378, 32.74466844138575, 27.479534403118993, 20.39075265780026, 20.069983672217962, 19.630457427805574, 24.717199695041675, 24.50550866373129, 20.033029690455415, 24.946001986379063, 31.183411355816336, 54.747594211743774, 38.46862212102038, 20.185095985556796, 18.21605788691756, 29.493453817221496, 22.763158340662596, 20.07297650422489, 40.42029477863873, 15.551281025913129, 14.852773203365098, 48.731737402742795, 15.249115743216986, 11.632023035885283, 38.42846801237138, 11.364211216869489, 12.481211777513904, 19.03774854138279, 10.466790557001204, 16.05745639125233, 21.765031667729094, 174.4421510805015, 167.40910133495657, 48.27737996564911, 52.319027839377604, 62.09172547690005, 45.076032494751445, 250.32149843876746, 74.04484062175585, 87.51658437792851, 47.249956163872305, 928.8074810225163, 474.1839087520481, 62.68003169172741, 48.12022743266021, 42.198221192865205, 227.23949475295882, 219.61089214168365, 204.8283608609497, 1474.2578338702683, 3619.6431233022186, 518.8758305634137, 1303.4608101622287, 617.5908160955827, 605.3755626401603, 898.1130671227528, 575.7622106047825, 4818.050203273673, 237.52147449575972, 191.31830695735457, 123.37666569032417, 135.6611957128199, 942.5795089500069, 225.26515794745694, 1192.506485215913, 242.06607959956543, 235.26915365735093, 376.7426546802747, 4966.51306925572, 780.2056638655654, 907.031282160595, 3814.444249640189, 1118.3823656673244, 673.1276958271491, 913.7985907107703, 780.4960254480046, 1266.8436947425107, 2956.135738033648, 1525.5096348171346, 1447.2491439912058, 4391.155539644378, 5284.590053770975, 792.6757359741824, 1668.7336684970574, 3846.2446384729233, 3777.442474449336, 4294.3671005526685, 3085.300789757747, 2535.0882009016823, 2911.908158374891, 1552.5747705731894, 1860.1850666748837, 1523.5176007166904, 1314.3511462254476, 49.19088314360904, 27.59623443921098, 23.07137562832867, 19.349566867954408, 39.48951754958744, 18.541991131795577, 17.67671213781327, 21.546101640638906, 52.95465059094762, 15.346437404300353, 61.52787647949526, 30.475482678449197, 16.94566226889518, 15.598624778262476, 23.014351586353378, 41.535481076639535, 27.11221629227582, 12.984442668527217, 12.644916622016101, 28.681656846538754, 12.427042353968409, 22.6464557149043, 15.134264868953982, 12.198987902387476, 11.90196360461896, 11.623276220148345, 17.101672137936976, 53.250180065292646, 17.4710909834851, 38.04393686871602, 33.163199466601526, 25.51033612457488, 263.57948038888776, 70.02197956581458, 46.34895240231001, 134.294332248687, 139.0208038558526, 23.075417336249615, 65.13176992306776, 138.79249563290819, 180.57490216742406, 169.6054606772453, 98.75686918040913, 37.2333726965873, 1041.1131466083853, 81.55366110784426, 1729.5542475584966, 169.33910037947916, 3846.2446384729233, 1668.7336684970574, 1101.560300028789, 3777.442474449336, 805.4533479920764, 383.4716041892367, 4966.51306925572, 1470.7775905972046, 3085.300789757747, 1456.7075464620177, 1525.3600599659012, 1418.5901242369794, 4391.155539644378, 1051.8478422550227, 3619.6431233022186, 2911.908158374891, 1526.1645906218696, 1462.9094069026942, 1397.574588860101, 2956.135738033648, 1616.7674815142532, 5284.590053770975, 1095.8563582998133, 2008.2480424916394, 3814.444249640189, 1239.7102477672208, 1025.5618258833308, 967.6314612577171, 4818.050203273673, 1774.5267834746537, 4294.3671005526685, 58.57036205690069, 35.21107512182105, 31.59636493423702, 55.400890352044264, 25.066799541497385, 25.034506392712302, 66.97707215798607, 88.27377788138934, 29.24039753735822, 90.14559814361203, 16.85170926663602, 15.720735045849155, 21.671794123034726, 30.120898669973684, 15.005733329334566, 21.341572611661572, 29.574959875137125, 15.319506719295585, 45.95355215403155, 18.573537920262073, 78.9064397473739, 18.998228011876137, 15.089228827583803, 13.522304412247978, 29.048742956558325, 12.716807314696208, 14.491178052048799, 24.526236106861994, 13.458770838156829, 11.984277435956628, 421.33420330526167, 52.96312874387682, 77.37173456913672, 66.38303279530129, 40.383217965687074, 703.0495948157815, 93.09076236051192, 103.7105313694716, 89.72855266443479, 36.80704681495499, 79.2849672308019, 71.13887555729508, 381.32010001078646, 140.2175561613029, 208.5842057365187, 818.4121068183907, 407.7160857516871, 228.26770657213098, 183.29572405499678, 526.0307472901299, 387.56447749218916, 405.93225475354245, 391.5604632249226, 1143.4296564816955, 1525.3600599659012, 1514.685121683172, 1407.1694031466918, 462.50313788836604, 133.62291654467674, 4391.155539644378, 627.7304076575808, 1526.1645906218696, 374.4612849491912, 1571.7005828049669, 646.5843205865931, 950.4460011430314, 1468.8571679131855, 1678.6709680911338, 824.3492107764725, 669.78623560867, 3777.442474449336, 3085.300789757747, 968.6205973206036, 1456.7075464620177, 1462.9094069026942, 2008.2480424916394, 4294.3671005526685, 2911.908158374891, 1774.5267834746537, 5284.590053770975, 1470.7775905972046, 3846.2446384729233, 1593.7893840406025, 1616.7674815142532, 3814.444249640189, 2956.135738033648, 2269.8404902658767, 4818.050203273673, 3619.6431233022186, 4966.51306925572], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -8.082599639892578, -8.212599754333496, -8.398599624633789, -8.777899742126465, -8.491600036621094, -9.02079963684082, -5.56689977645874, -9.039400100708008, -9.032299995422363, -8.501399993896484, -8.841899871826172, -8.983799934387207, -8.880599975585938, -9.062399864196777, -9.162699699401855, -7.176700115203857, -8.990699768066406, -9.233200073242188, -8.664899826049805, -9.261899948120117, -8.60420036315918, -9.302300453186035, -8.14680004119873, -9.230799674987793, -9.164799690246582, -9.258500099182129, -8.710399627685547, -9.405099868774414, -9.376099586486816, -9.332900047302246, -7.319300174713135, -9.099100112915039, -8.98449993133545, -7.812300205230713, -7.59660005569458, -8.258600234985352, -7.816299915313721, -8.28380012512207, -6.0457000732421875, -8.48169994354248, -8.09280014038086, -5.740200042724609, -7.128200054168701, -8.177399635314941, -7.958399772644043, -8.086799621582031, -6.583899974822998, -6.7195000648498535, -5.320899963378906, -7.021900177001953, -7.061399936676025, -5.090799808502197, -6.128900051116943, -5.004799842834473, -5.827899932861328, -6.030600070953369, -4.305600166320801, -6.022600173950195, -6.228000164031982, -5.73799991607666, -4.392099857330322, -5.622900009155273, -4.629199981689453, -5.467100143432617, -4.982699871063232, -4.602399826049805, -4.706299781799316, -4.914999961853027, -4.5690999031066895, -5.689300060272217, -6.007199764251709, -6.1209001541137695, -6.163599967956543, -5.669000148773193, -5.454400062561035, -5.825799942016602, -5.112299919128418, -5.732600212097168, -5.4720001220703125, -5.048500061035156, -5.01669979095459, -5.233099937438965, -5.5269999504089355, -5.600399971008301, -5.234899997711182, -5.633999824523926, -5.487100124359131, -5.620699882507324, -5.732699871063232, -7.081999778747559, -8.09179973602295, -8.23289966583252, -7.895599842071533, -8.172800064086914, -8.592900276184082, -7.348999977111816, -8.584500312805176, -8.7253999710083, -8.795499801635742, -8.763099670410156, -8.593700408935547, -8.530900001525879, -8.826399803161621, -8.611599922180176, -8.722599983215332, -8.673500061035156, -8.871700286865234, -8.83489990234375, -8.83590030670166, -8.48330020904541, -8.401599884033203, -8.785699844360352, -7.957699775695801, -8.083100318908691, -8.976699829101562, -8.928000450134277, -8.462599754333496, -8.414299964904785, -8.578300476074219, -7.370500087738037, -8.264200210571289, -6.450399875640869, -8.479499816894531, -7.464700222015381, -7.970300197601318, -7.438499927520752, -7.876999855041504, -8.030900001525879, -7.6442999839782715, -8.016799926757812, -7.069699764251709, -5.292200088500977, -5.603700160980225, -6.786200046539307, -7.1305999755859375, -7.303599834442139, -7.5320000648498535, -7.991600036621094, -7.372099876403809, -8.175299644470215, -7.708099842071533, -4.416200160980225, -6.965199947357178, -5.490099906921387, -4.906899929046631, -7.09630012512207, -5.571899890899658, -4.839900016784668, -4.254199981689453, -5.410399913787842, -5.567699909210205, -6.116700172424316, -5.947700023651123, -4.192399978637695, -5.948699951171875, -5.890399932861328, -4.794000148773193, -5.1890997886657715, -5.122300148010254, -5.68120002746582, -6.107800006866455, -4.5609002113342285, -5.394800186157227, -5.580699920654297, -4.924699783325195, -5.282599925994873, -5.512899875640869, -5.385799884796143, -4.884900093078613, -5.605299949645996, -5.097099781036377, -5.146100044250488, -5.18779993057251, -5.635700225830078, -5.581200122833252, -5.622300148010254, -5.606400012969971, -8.1875, -8.374600410461426, -8.677499771118164, -8.694600105285645, -8.717000007629395, -8.486700057983398, -8.497099876403809, -8.70009994506836, -8.482000350952148, -8.259099960327148, -7.696700096130371, -8.05780029296875, -8.705900192260742, -8.809200286865234, -8.327799797058105, -8.5871000289917, -8.714799880981445, -8.016500473022461, -8.973600387573242, -9.020999908447266, -7.833600044250488, -8.999099731445312, -9.270299911499023, -8.077300071716309, -9.298399925231934, -9.204999923706055, -8.785200119018555, -9.384699821472168, -8.958000183105469, -8.65429973602295, -6.58050012588501, -6.624899864196777, -7.860000133514404, -7.780700206756592, -7.611700057983398, -7.928800106048584, -6.245800018310547, -7.439899921417236, -7.276199817657471, -7.894499778747559, -5.039700031280518, -5.684100151062012, -7.63700008392334, -7.899499893188477, -8.021699905395508, -6.5081000328063965, -6.558499813079834, -6.632400035858154, -4.905200004577637, -4.133399963378906, -5.831600189208984, -5.045899868011475, -5.69379997253418, -5.711100101470947, -5.392499923706055, -5.7743000984191895, -4.013299942016602, -6.547800064086914, -6.7316999435424805, -7.097400188446045, -7.020500183105469, -5.444699764251709, -6.611400127410889, -5.280600070953369, -6.557400226593018, -6.593599796295166, -6.2368998527526855, -4.271299839019775, -5.692500114440918, -5.587800025939941, -4.548900127410889, -5.448599815368652, -5.822000026702881, -5.608500003814697, -5.740099906921387, -5.410200119018555, -4.861999988555908, -5.3130998611450195, -5.360599994659424, -4.739299774169922, -4.658400058746338, -5.792799949645996, -5.402900218963623, -4.981400012969971, -5.053100109100342, -5.0040998458862305, -5.245200157165527, -5.336900234222412, -5.327099800109863, -5.52869987487793, -5.5609002113342285, -5.596199989318848, -5.63129997253418, -7.165500164031982, -7.751399993896484, -7.935400009155273, -8.113300323486328, -7.401899814605713, -8.15999984741211, -8.208100318908691, -8.011099815368652, -7.112299919128418, -8.355400085449219, -6.969099998474121, -7.672399997711182, -8.261500358581543, -8.345100402832031, -7.956699848175049, -7.367300033569336, -7.796599864959717, -8.534899711608887, -8.561800003051758, -7.743800163269043, -8.580300331115723, -7.981599807739258, -8.385000228881836, -8.60099983215332, -8.6274995803833, -8.652000427246094, -8.265899658203125, -7.130199909210205, -8.245699882507324, -7.469299793243408, -7.609899997711182, -7.873799800872803, -5.605599880218506, -6.909900188446045, -7.309800148010254, -6.2957000732421875, -6.292399883270264, -7.988999843597412, -7.043499946594238, -6.358399868011475, -6.223299980163574, -6.305099964141846, -6.789000034332275, -7.610899925231934, -5.139100074768066, -7.045400142669678, -4.793700218200684, -6.522200107574463, -4.346199989318848, -5.089600086212158, -5.3668999671936035, -4.587399959564209, -5.599100112915039, -6.0929999351501465, -4.5081000328063965, -5.267300128936768, -4.82420015335083, -5.373300075531006, -5.3516998291015625, -5.395599842071533, -4.7804999351501465, -5.604700088500977, -4.996099948883057, -5.110799789428711, -5.4440999031066895, -5.473700046539307, -5.504700183868408, -5.187699794769287, -5.466599941253662, -4.935400009155273, -5.680799961090088, -5.457699775695801, -5.263999938964844, -5.664000034332275, -5.723899841308594, -5.740600109100342, -5.295199871063232, -5.5767998695373535, -5.4542999267578125, -7.716800212860107, -8.237799644470215, -8.347399711608887, -7.7870001792907715, -8.58329963684082, -8.58530044555664, -7.603000164031982, -7.330900192260742, -8.4399995803833, -7.315299987792969, -8.997599601745605, -9.0693998336792, -8.749300003051758, -8.421199798583984, -9.118399620056152, -8.766599655151367, -8.440799713134766, -9.100000381469727, -8.001500129699707, -8.908599853515625, -7.462900161743164, -8.888199806213379, -9.119999885559082, -9.23169994354248, -8.467700004577637, -9.295900344848633, -9.166399955749512, -8.64050006866455, -9.242300033569336, -9.358599662780762, -5.811200141906738, -7.888700008392334, -7.529699802398682, -7.684299945831299, -8.168000221252441, -5.41349983215332, -7.3592000007629395, -7.265900135040283, -7.411399841308594, -8.258299827575684, -7.534299850463867, -7.63670015335083, -6.0696001052856445, -7.031000137329102, -6.6707000732421875, -5.4375, -6.0742998123168945, -6.60290002822876, -6.805099964141846, -5.8678998947143555, -6.146200180053711, -6.105999946594238, -6.140999794006348, -5.20419979095459, -5.052700042724609, -5.061600208282471, -5.146299839019775, -6.076900005340576, -7.110499858856201, -4.272600173950195, -5.859099864959717, -5.156099796295166, -6.27839994430542, -5.155799865722656, -5.856599807739258, -5.561299800872803, -5.2845001220703125, -5.191400051116943, -5.724699974060059, -5.8815999031066895, -4.643199920654297, -4.797900199890137, -5.623799800872803, -5.360300064086914, -5.377500057220459, -5.181000232696533, -4.696899890899658, -4.991099834442139, -5.2881999015808105, -4.703499794006348, -5.454800128936768, -5.114999771118164, -5.497399806976318, -5.509699821472168, -5.230899810791016, -5.321499824523926, -5.429200172424316, -5.26230001449585, -5.470300197601318, -5.551199913024902], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3037, 1.296, 1.2914, 1.2881, 1.2815, 1.2778, 1.2775, 1.2765, 1.276, 1.2755, 1.274, 1.2738, 1.2711, 1.2708, 1.2673, 1.267, 1.2669, 1.265, 1.2616, 1.261, 1.2595, 1.2593, 1.2592, 1.258, 1.2578, 1.2568, 1.2539, 1.2534, 1.2528, 1.2527, 1.2515, 1.2524, 1.2494, 1.2367, 1.2269, 1.2391, 1.2173, 1.2253, 1.1029, 1.2237, 1.1992, 1.0454, 1.1265, 1.2005, 1.1783, 1.1879, 1.0274, 0.9978, 0.7834, 0.9991, 1.0028, 0.6651, 0.8276, 0.5704, 0.6459, 0.6608, 0.2254, 0.646, 0.6969, 0.566, 0.201, 0.533, 0.2278, 0.4553, 0.2829, 0.1361, 0.1424, 0.212, 0.0543, 0.4351, 0.5699, 0.6121, 0.6287, 0.3507, 0.2293, 0.4304, -0.0432, 0.3479, 0.1599, -0.1818, -0.3005, -0.1212, 0.0952, 0.1744, -0.3255, 0.1921, -0.111, -0.1221, 0.0407, 2.1549, 2.1472, 2.1403, 2.1289, 2.1209, 2.118, 2.115, 2.1127, 2.107, 2.1061, 2.106, 2.103, 2.1029, 2.1016, 2.1004, 2.0973, 2.0962, 2.0927, 2.0917, 2.0916, 2.0895, 2.0892, 2.089, 2.0868, 2.0852, 2.085, 2.0848, 2.0802, 2.0774, 2.0769, 2.0681, 2.0767, 1.9499, 2.0705, 2.0025, 2.0346, 1.9924, 2.007, 2.0114, 1.9488, 1.9947, 1.85, 1.5541, 1.5506, 1.7455, 1.801, 1.8263, 1.8677, 1.9742, 1.8221, 2.0142, 1.8963, 0.9599, 1.6617, 1.156, 0.9367, 1.6809, 1.1195, 0.792, 0.4843, 0.9786, 1.019, 1.2359, 1.1575, 0.3386, 1.1259, 1.0621, 0.4716, 0.6594, 0.5931, 0.864, 1.1041, 0.0322, 0.5644, 0.6756, 0.1872, 0.4471, 0.6114, 0.4219, -0.2615, 0.4751, -0.2304, -0.2891, -0.3391, 0.4308, 0.265, 0.2216, -0.8902, 1.4273, 1.4155, 1.411, 1.4097, 1.4095, 1.4094, 1.4076, 1.4061, 1.4048, 1.4046, 1.4041, 1.3959, 1.3927, 1.3921, 1.3916, 1.3913, 1.3894, 1.3878, 1.3858, 1.3843, 1.3836, 1.38, 1.3795, 1.3775, 1.3747, 1.3743, 1.3719, 1.3707, 1.3693, 1.369, 1.3614, 1.3583, 1.3666, 1.3655, 1.3632, 1.3664, 1.335, 1.359, 1.3555, 1.3536, 1.2299, 1.2579, 1.3285, 1.3304, 1.3395, 1.1695, 1.1532, 1.149, 0.9024, 0.7761, 1.0203, 0.8849, 0.984, 0.9866, 0.9107, 0.9736, 0.6101, 1.0855, 1.118, 1.1909, 1.1729, 0.8102, 1.0749, 0.7392, 1.0569, 1.0492, 0.9351, 0.3218, 0.7515, 0.7056, 0.3081, 0.6353, 0.7696, 0.6774, 0.7035, 0.5491, 0.2499, 0.4603, 0.4655, -0.0231, -0.1274, 0.6354, 0.2809, -0.1327, -0.1863, -0.2656, -0.1761, -0.0714, -0.2001, 0.2272, 0.0142, 0.1785, 0.2912, 2.0423, 2.0345, 2.0296, 2.0275, 2.0256, 2.0235, 2.0232, 2.0223, 2.0218, 2.0173, 2.015, 2.0143, 2.0121, 2.0113, 2.0108, 2.0097, 2.007, 2.0049, 2.0045, 2.0035, 2.0034, 2.002, 2.0016, 2.0012, 1.9993, 1.9985, 1.9985, 1.9983, 1.9973, 1.9956, 1.9923, 1.9906, 1.9236, 1.9448, 1.9575, 1.9078, 1.8765, 1.9758, 1.8837, 1.8122, 1.6842, 1.6649, 1.7219, 1.8755, 1.0164, 1.6569, 0.8542, 1.4495, 0.5025, 0.5942, 0.7322, 0.2794, 0.813, 1.0613, 0.085, 0.5427, 0.2449, 0.4463, 0.4219, 0.4505, -0.0643, 0.5405, -0.0867, 0.0162, 0.3289, 0.3417, 0.3563, -0.0757, 0.2488, -0.4044, 0.4234, 0.0408, -0.407, 0.3169, 0.4467, 0.4881, -0.6717, 0.0455, -0.7158, 1.3165, 1.3044, 1.3031, 1.302, 1.2987, 1.298, 1.2962, 1.2922, 1.288, 1.2868, 1.2815, 1.2792, 1.2782, 1.2771, 1.2767, 1.2763, 1.2759, 1.2745, 1.2744, 1.2733, 1.2724, 1.271, 1.2696, 1.2675, 1.2669, 1.2647, 1.2637, 1.2633, 1.2616, 1.2613, 1.249, 1.2453, 1.2252, 1.2239, 1.2371, 1.1347, 1.2108, 1.196, 1.1953, 1.2395, 1.1962, 1.2022, 1.0903, 1.1293, 1.0925, 0.9587, 1.0187, 1.0701, 1.0873, 0.9703, 0.9975, 0.9913, 0.9924, 0.8576, 0.7209, 0.719, 0.7079, 0.89, 1.0981, 0.4436, 0.8024, 0.617, 0.8997, 0.5879, 0.7753, 0.6853, 0.5268, 0.4864, 0.6643, 0.715, 0.2235, 0.2713, 0.6039, 0.4593, 0.4379, 0.3176, 0.0416, 0.1359, 0.3341, -0.1725, 0.3552, -0.2663, 0.2323, 0.2056, -0.3739, -0.2096, -0.0531, -0.6388, -0.5609, -0.9582]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 1, 1, 5, 5, 2, 4, 5, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 2, 4, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 1, 2, 3, 4, 5, 2, 5, 2, 2, 1, 2, 3, 4, 5, 2, 4, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 3, 5, 3, 5, 1, 4, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 5, 2, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 3, 4, 5, 2, 3, 4, 5, 2, 5, 1, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 5, 2, 1, 2, 3, 4, 5, 1, 2, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 4, 3, 2, 5, 2, 3, 1, 4, 1, 3, 4, 4, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 2, 5, 1, 2, 3, 4, 5, 2, 1, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 2, 1, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 2, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 1, 5, 1, 3, 4, 5, 1, 3, 4, 5, 1, 2, 3, 5, 4, 3, 1, 2, 4, 1, 2, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 4, 1, 2, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 4, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 4, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 2, 4, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 2, 3, 4, 5, 1, 3, 4, 3, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 1, 3, 4, 5, 2, 1, 2, 3, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 3, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 3, 4, 1, 4, 1, 2, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 5, 5, 3, 4, 1, 2, 3, 4, 5, 3, 1, 2, 3, 3, 1, 2, 3, 4, 5, 5, 1, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 5, 1, 2, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 4, 5, 1, 1, 3, 4, 5, 1, 2, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 3, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 5, 1, 2, 3, 4, 5, 2, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 5, 1, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 4, 1, 1, 2, 3, 4, 5, 3, 1, 2, 3, 4, 2, 4, 1, 5, 3, 1, 3, 5, 1, 1, 3, 4, 5, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 3, 4, 1, 2, 3, 4, 5, 1, 1, 4, 5, 3, 3, 5, 1, 4], \"Freq\": [0.2633133164650272, 0.03863836708997682, 0.2375544050717093, 0.17959685443674409, 0.28048592406057243, 0.9396228593039896, 0.9018661403029689, 0.9613746003399468, 0.943633075743168, 0.7808582355594922, 0.13014303925991536, 0.043381013086638456, 0.9590253692561479, 0.922543334528715, 0.04393063497755785, 0.16746153867862182, 0.2148029863219453, 0.22964135050865864, 0.14909023063792914, 0.23953359329980084, 0.011093165063998888, 0.011093165063998888, 0.011093165063998888, 0.011093165063998888, 0.9540121955039044, 0.01532330396839234, 0.010215535978928228, 0.010215535978928228, 0.2502806314837416, 0.7125336345302439, 0.9014141439017598, 0.9659128724551745, 0.1343930270696862, 0.6306134347116046, 0.05168962579603316, 0.06202755095523979, 0.11371717675127295, 0.914897539973911, 0.20873312318781226, 0.024476848321697855, 0.17337767561202647, 0.2168920726283782, 0.37667149917279474, 0.045111653019732946, 0.857121407374926, 0.09022330603946589, 0.9083455302132593, 0.9616232336650157, 0.18479698421017632, 0.3075021817257334, 0.13157545275764554, 0.11383494227346862, 0.2631509055152911, 0.8996906311402989, 0.013841394325235368, 0.027682788650470736, 0.05536557730094147, 0.9539387502530664, 0.06600965213510575, 0.08361222603780062, 0.7525100343402056, 0.017602573902694867, 0.07481093908645318, 0.40966747043741425, 0.20483373521870712, 0.18807461142808563, 0.10707217977341509, 0.0912441184156059, 0.929304388800437, 0.3878902315605966, 0.3218210162947807, 0.22804535591749361, 0.02557518010289648, 0.03836277015434472, 0.5138067540789328, 0.3398019108445338, 0.07551153574322975, 0.03447265762190923, 0.03611421274676205, 0.057340514988355115, 0.01911350499611837, 0.9174482398136818, 0.01911350499611837, 0.18502091730636744, 0.5947100913418953, 0.013215779807597673, 0.013215779807597673, 0.1982366971139651, 0.04831561656498157, 0.9179967147346497, 0.01610520552166052, 0.01610520552166052, 0.9659131419913001, 0.035774560814492595, 0.37514088871217116, 0.17821419891314544, 0.3305873389838848, 0.05791961464677227, 0.05791961464677227, 0.03047899747127597, 0.7010169418393473, 0.13715548862074187, 0.045718496206913956, 0.09143699241382791, 0.9031851699959953, 0.060212344666399686, 0.9720221570376221, 0.14647049548921273, 0.028908650425502513, 0.6494810128929565, 0.01734519025530151, 0.15610671229771356, 0.8131241263041906, 0.1778709026290417, 0.8755618692313484, 0.9078588404937238, 0.0732320462701107, 0.014646409254022141, 0.7372025991191145, 0.1269355468681919, 0.0488213641800738, 0.9029781913877306, 0.05311636419927827, 0.24489276545588698, 0.09031506243763208, 0.6200476401968202, 0.024315593733208636, 0.020841937485607403, 0.9656052785201518, 0.11119092599175504, 0.018328174614025555, 0.025659444459635777, 0.15517854506541637, 0.6879174871797592, 0.030153906018841593, 0.030153906018841593, 0.9347710865840894, 0.030153906018841593, 0.924217243928647, 0.11521235571044494, 0.01137899809485876, 0.009956623333001414, 0.04267124285572035, 0.820710237591688, 0.08510177870138089, 0.0298602732285547, 0.09555287433137503, 0.24933328145843173, 0.5389779317754123, 0.052527219688098994, 0.9454899543857819, 0.06462308267797941, 0.03877384960678765, 0.9047231574917117, 0.9189051642710864, 0.045945258213554316, 0.9661251513753384, 0.9241829092200167, 0.8643200396527057, 0.08643200396527057, 0.31416919785231456, 0.08060920208052809, 0.10954583872482022, 0.20462335912749438, 0.290399817751646, 0.22046221934430182, 0.12136556519459041, 0.5823320238235852, 0.03785714877629425, 0.0389705943285382, 0.13498317283997666, 0.16889854290027229, 0.5772395984262317, 0.07054396972541493, 0.04748151808441389, 0.14308537855928471, 0.3815610094914259, 0.4038187350450924, 0.01589837539547608, 0.05723415142371389, 0.052525807408306174, 0.919201629645358, 0.08465616319573291, 0.9100537543541288, 0.039427390475647785, 0.01752328465584346, 0.030665748147726053, 0.14456709841070856, 0.7710245248571123, 0.026348130902302026, 0.8167920579713628, 0.026348130902302026, 0.13174065451151012, 0.026348130902302026, 0.30549278853632955, 0.07903813422556952, 0.20513515757886297, 0.2079950900672882, 0.20227522509043777, 0.8558687592348108, 0.07780625083952825, 0.07780625083952825, 0.946376889928166, 0.11037092126751781, 0.029432245671338085, 0.029432245671338085, 0.10056017271040513, 0.7309007675048957, 0.04279064736442664, 0.014263549121475547, 0.04279064736442664, 0.08558129472885329, 0.8130222999241061, 0.9808367712385162, 0.8915797669484011, 0.024096750458064895, 0.024096750458064895, 0.04819350091612979, 0.04819350091612979, 0.9620499713032324, 0.032068332376774415, 0.9171494525035744, 0.9400291719526078, 0.03916788216469199, 0.03916788216469199, 0.7190883720628299, 0.03239136811093828, 0.18786993504344204, 0.03239136811093828, 0.025913094488750628, 0.15706192105192768, 0.08743653336911439, 0.6266284891453198, 0.012953560499128058, 0.11496284942976151, 0.14115875445740694, 0.08651665595776555, 0.7422218379534623, 0.013660524624910348, 0.018214032833213798, 0.04778712708094279, 0.023893563540471396, 0.913928805423031, 0.005973390885117849, 0.011946781770235698, 0.1850593899423834, 0.6785510964554058, 0.0925296949711917, 0.015421615828531951, 0.04626484748559585, 0.04685690310626757, 0.9371380621253513, 0.1338079689753519, 0.05159915143493963, 0.05597196087857858, 0.13643165464153528, 0.6226880647741867, 0.97742554866817, 0.2500476263097292, 0.029267661094777467, 0.11388937686880797, 0.13170447492649862, 0.47528136603910365, 0.08935596759048872, 0.02233899189762218, 0.029785322530162906, 0.8488816921096428, 0.0074463306325407265, 0.9074885797633957, 0.05671803623521223, 0.2722132545265607, 0.13856850424220274, 0.27995058217460245, 0.0991784725794446, 0.21031463334222647, 0.9157985620431099, 0.31653470879176165, 0.19497275080190804, 0.40573276887522564, 0.03315326126996007, 0.04972989190494011, 0.018884082679056443, 0.9442041339528221, 0.018884082679056443, 0.015353490334765635, 0.07676745167382817, 0.8290884780773443, 0.0921209420085938, 0.033199540656363616, 0.9627866790345447, 0.032813262075325624, 0.032813262075325624, 0.9515846001844431, 0.9192419404651484, 0.017344187555946196, 0.017344187555946196, 0.03468837511189239, 0.03468837511189239, 0.342876818161312, 0.0948148114529673, 0.47407405726483653, 0.05402239257203951, 0.033074934227779296, 0.014386336034092805, 0.014386336034092805, 0.8200211539432899, 0.14386336034092803, 0.17983296263146467, 0.012317326207634567, 0.08375781821191505, 0.012317326207634567, 0.711941454801278, 0.9586767809005632, 0.9071063460597903, 0.0037939220402308637, 0.0037939220402308637, 0.01896961020115432, 0.861220303132406, 0.11381766120692591, 0.9027523068166873, 0.03343527062284027, 0.03343527062284027, 0.03343527062284027, 0.9691206961902291, 0.27597334713029587, 0.0668629012049648, 0.3710235498236282, 0.13634787696698705, 0.14945824975227426, 0.03222661329576314, 0.010742204431921046, 0.05371102215960523, 0.010742204431921046, 0.8808607634175257, 0.8650744011016777, 0.9385644801587213, 0.515273090328945, 0.04379821267796032, 0.2937056614874986, 0.08179960308972001, 0.0650532276540293, 0.05904881942048621, 0.04850438738111367, 0.8245745854789325, 0.02319775048661958, 0.04428661456536466, 0.09728870119111237, 0.007483746245470182, 0.007483746245470182, 0.09728870119111237, 0.7932771020198393, 0.02532317592242773, 0.02532317592242773, 0.9622806850522538, 0.9772583300784147, 0.962645686462148, 0.020927080140481478, 0.9270063115877012, 0.07725052596564176, 0.0348654893038607, 0.941368211204239, 0.03250559119599258, 0.01625279559799629, 0.9426621446837847, 0.910059380248378, 0.043336160964208476, 0.967566061331841, 0.03919979709858349, 0.9407951303660037, 0.03919979709858349, 0.17093341803902878, 0.061783163146636906, 0.1523984690950377, 0.19701964247871992, 0.4180660706255764, 0.014930482443920387, 0.9704813588548251, 0.24441195073261507, 0.12502921572130474, 0.18310730302410436, 0.17262098170554332, 0.27506427458687044, 0.9127190572723467, 0.9095889588829509, 0.04331375994680718, 0.2315236904543214, 0.17379962670961252, 0.17505449766058445, 0.08658609561706328, 0.33316823748304786, 0.7986275528922107, 0.03111535920359262, 0.10890375721257417, 0.023336519402694465, 0.036301252404191395, 0.09703133928411688, 0.015734811775802736, 0.03409209218090593, 0.06818418436181185, 0.7841181201608364, 0.2524975261317052, 0.11535419975560136, 0.4960230589490858, 0.04870510656347613, 0.08715650648200991, 0.3239697505197795, 0.11456730145789591, 0.3231643564849261, 0.13711833443379107, 0.10127829988281484, 0.021761103399539246, 0.021761103399539246, 0.9357274461801875, 0.40793312288358147, 0.2177767973727453, 0.2708930894148783, 0.02443349433938118, 0.07861211222235684, 0.20743223041347922, 0.2833735215479055, 0.1561015428874318, 0.1096929760830602, 0.24399655577449927, 0.06312561078012247, 0.8837585509217145, 0.9306154872048686, 0.9604475134328525, 0.37651786851180513, 0.07714842740108256, 0.49056336988731847, 0.02431852602860211, 0.03270422465915456, 0.9645507643393153, 0.7412927830687042, 0.04118293239270579, 0.049419518871246945, 0.078247571546141, 0.09060245126395274, 0.32762022293052623, 0.0504823613949553, 0.19162692284615687, 0.12809469932189338, 0.30220733352082085, 0.21354148962338007, 0.03607778591810531, 0.14138591778716944, 0.1969652096069533, 0.4114817745254173, 0.04051598970041594, 0.013505329900138648, 0.918362433209428, 0.027010659800277295, 0.25378400789943073, 0.04278351091024886, 0.19641520917886976, 0.16076228342032906, 0.3461574973647408, 0.27978122749407797, 0.08551176982756685, 0.43108724740741833, 0.06434138021006243, 0.13926795522887708, 0.3036535930596572, 0.18000324189809433, 0.17953751552837094, 0.06147588080348888, 0.27524428450652977, 0.20686472876229506, 0.6958177240186288, 0.05641765329880775, 0.03761176886587183, 0.018805884432935915, 0.9085347962535928, 0.016613889746160522, 0.033227779492321044, 0.014240476925280448, 0.016613889746160522, 0.920884174501469, 0.01114472450859407, 0.08915779606875256, 0.01114472450859407, 0.01114472450859407, 0.8692885116703374, 0.9489979537790518, 0.9811255207528378, 0.8612203882759532, 0.033812387378441486, 0.9467468465963617, 0.9691459404694579, 0.09558243358624943, 0.12903628534143674, 0.04460513567358307, 0.14337365037937413, 0.5894250071152048, 0.05538915930445134, 0.3419678530970474, 0.07224672952754523, 0.08910429975063912, 0.4407050501180259, 0.9589144355645122, 0.9425849171943264, 0.9474567832719911, 0.5328277748860357, 0.024879117893511396, 0.07878387332945275, 0.2446446592861954, 0.1223223296430977, 0.9329767291433556, 0.10737136417684649, 0.08094148991793043, 0.6277095136492564, 0.014866804270640284, 0.16849044840058988, 0.9576827232994065, 0.9292930475624306, 0.9138675452497691, 0.018881059780963414, 0.018881059780963414, 0.03776211956192683, 0.9251719292672073, 0.04050351163616575, 0.010125877909041438, 0.7088114536329007, 0.2430210698169945, 0.14941428754657574, 0.7769542952421938, 0.029882857509315148, 0.029882857509315148, 0.9617173073511971, 0.9825493985420449, 0.046000188502975915, 0.9200037700595183, 0.9441944343107201, 0.9776550484489085, 0.9138503770919543, 0.9373409496630409, 0.018936180801273555, 0.018936180801273555, 0.009468090400636777, 0.018936180801273555, 0.9819341243997901, 0.1349730987508992, 0.07033809371525733, 0.028515443398077295, 0.06843706415538552, 0.6957768189130861, 0.9337489876274023, 0.035913422601053936, 0.09082352588224424, 0.8174117329401981, 0.04541176294112212, 0.04541176294112212, 0.9234812306478832, 0.2227952988460189, 0.07148512262439109, 0.15011875751122128, 0.12629038330309092, 0.4295064451015498, 0.03411515196761253, 0.579957583449413, 0.23880606377328767, 0.11371717322537508, 0.03411515196761253, 0.08306802229964613, 0.04984081337978768, 0.16613604459929227, 0.6756199147037886, 0.02215147261323897, 0.4975340796561819, 0.0835276192123517, 0.27600430696255346, 0.03631635617928335, 0.10894906853785005, 0.0341992614403541, 0.9575793203299149, 0.951469158308691, 0.057124920272217494, 0.014281230068054374, 0.028562460136108747, 0.8854362642193712, 0.014281230068054374, 0.23387413095092588, 0.02757052763242622, 0.12834555966819103, 0.21581068181243973, 0.3935930496491192, 0.9707695291221353, 0.02407580155758499, 0.02407580155758499, 0.9389562607458146, 0.9067227364516148, 0.08242933967741953, 0.33268280172654324, 0.08310516008456596, 0.3187882481477357, 0.08389164424940412, 0.1816778420776158, 0.32129920541367796, 0.11884615770140346, 0.37312165789975504, 0.05942307885070173, 0.1278287161323235, 0.7196759486818244, 0.031020515029388983, 0.1923271931822117, 0.04963282404702237, 0.012408206011755593, 0.3066357212207447, 0.3342605609703613, 0.1740364904225848, 0.04972471154930994, 0.1353617147731215, 0.23453278182491286, 0.0677207183188708, 0.2041580478730664, 0.1309599185136987, 0.36250502158924963, 0.2370210604256961, 0.2898882114500149, 0.18151055185016132, 0.04141260163571642, 0.2502378481817758, 0.5057625892710924, 0.05414168710212739, 0.27467002237176824, 0.04093639756502315, 0.1254502506024903, 0.9286611655256323, 0.03012818058745824, 0.01506409029372912, 0.03012818058745824, 0.03012818058745824, 0.9038454176237471, 0.9282421634119303, 0.9498134255704994, 0.018265642799432682, 0.018265642799432682, 0.023584146312434443, 0.106128658405955, 0.017688109734325833, 0.6662521333262731, 0.18867317049947555, 0.8845292550879632, 0.022113231377199078, 0.06633969413159724, 0.044226462754398156, 0.28671525942736603, 0.0776976466387137, 0.46071421457603473, 0.1159993034324458, 0.0579996517162229, 0.054251800038314035, 0.8137770005747106, 0.054251800038314035, 0.054251800038314035, 0.41769659620765553, 0.12858055511674643, 0.313462655077512, 0.0555407131569378, 0.08521314895311005, 0.8807416852107557, 0.07339514043422964, 0.9009505769531503, 0.14038647693493014, 0.6818771736839464, 0.020055210990704304, 0.14038647693493014, 0.020055210990704304, 0.1431160488995788, 0.010565614348291051, 0.14983962166667308, 0.3477047630983055, 0.3486652734936047, 0.020520507851700696, 0.020520507851700696, 0.943943361178232, 0.020520507851700696, 0.1124316696258852, 0.03243221239208227, 0.08648589971221939, 0.1275667020755236, 0.642157805363229, 0.047395362730080104, 0.023697681365040052, 0.900511891871522, 0.023697681365040052, 0.023697681365040052, 0.8723121363544805, 0.013038775088891811, 0.5241587585734508, 0.005215510035556725, 0.3650857024889707, 0.09387918064002104, 0.16629127912867675, 0.0717752956922921, 0.11441408521246563, 0.11228214573645695, 0.5358274549701807, 0.2302212852649726, 0.17046919595955987, 0.10544486348014012, 0.11598934982815413, 0.37696538694150095, 0.31066211063877514, 0.2452296148082639, 0.1695555109347161, 0.08250184256890548, 0.19231463991924178, 0.451607996064383, 0.12106937766832396, 0.22003878957179512, 0.07686944613861839, 0.13067805843565125, 0.9611351950605684, 0.04683184249756809, 0.8429731649562257, 0.09366368499513618, 0.020713634433176194, 0.9113999150597526, 0.020713634433176194, 0.04142726886635239, 0.19003075555276017, 0.07849096425005311, 0.673369851197824, 0.020655516907908712, 0.03717993043423568, 0.4685276258831106, 0.10102626933104572, 0.15959222256643454, 0.10981116231635404, 0.16105637139731926, 0.19145533865995404, 0.03370498025471052, 0.5088899477801375, 0.11548099792187703, 0.15056732982637078, 0.901714149404737, 0.046932652954485704, 0.8917204061352283, 0.046932652954485704, 0.06484208302467848, 0.01621052075616962, 0.7699997359180568, 0.01621052075616962, 0.12968416604935695, 0.011465118881026873, 0.022930237762053746, 0.9114769510416364, 0.01719767832154031, 0.028662797202567182, 0.04345114813458231, 0.9559252589608108, 0.02952655345868313, 0.05905310691736626, 0.05314779622562963, 0.5373832729480329, 0.3247920880455144, 0.02602237486225231, 0.9368054950410831, 0.02602237486225231, 0.9620780120639935, 0.014410000993784228, 0.18733001291919496, 0.007205000496892114, 0.7709350531674561, 0.02161500149067634, 0.9678837118226006, 0.2539389180569023, 0.07216494722260494, 0.11914024305618741, 0.10824742083390741, 0.4472865125023721, 0.024740047183636052, 0.9401217929781699, 0.024740047183636052, 0.024740047183636052, 0.9634575461011616, 0.06749555621369163, 0.03856888926496665, 0.019284444632483324, 0.8774422307779913, 0.0906621237803791, 0.7796942645112602, 0.11786076091449284, 0.00906621237803791, 0.14965251814433617, 0.010320863320299046, 0.025802158300747613, 0.1006284173729157, 0.7147197849307089, 0.8693781419328098, 0.08693781419328098, 0.04346890709664049, 0.09151519742974532, 0.036606078971898125, 0.8010271398556531, 0.05060252093174152, 0.020456338249001893, 0.9484336764624477, 0.331908432281967, 0.15573582654963447, 0.20626008619574918, 0.08401786997331434, 0.22215535889340324, 0.9358276438065752, 0.025995212327960423, 0.9403433131871647, 0.9250531903085175, 0.29197649544979787, 0.05043230375951054, 0.5972246497836775, 0.03716064487542882, 0.0238889859913471, 0.8880831176672147, 0.08539260746800141, 0.01707852149360028, 0.2110190640024817, 0.08136752044163488, 0.4426035452594425, 0.07510848040766298, 0.18955949817172082, 0.46877056246812937, 0.08440020448107376, 0.2376107667556344, 0.060746643989562636, 0.1483723339922061, 0.9614451075671312, 0.05875106689062681, 0.06676257601207591, 0.06676257601207591, 0.15755967938849919, 0.6489322388373779, 0.9494585829152487, 0.036883742340344805, 0.9220935585086202, 0.0415625633274243, 0.02078128166371215, 0.8935951115396225, 0.0415625633274243, 0.02078128166371215, 0.9180860212322403, 0.07333473965768877, 0.8800168758922653, 0.9456652523868406, 0.9554027039655466, 0.1332308725197547, 0.1445096765425911, 0.19949384615391844, 0.19737907039963662, 0.3249705409079731, 0.9574417332483419, 0.08828912017611994, 0.02522546290746284, 0.01261273145373142, 0.870278470307468, 0.052736495369621036, 0.04314804166605358, 0.009588453703567461, 0.1102672175910258, 0.7862532036925318, 0.33318678138780566, 0.08389595214800862, 0.3104150229476319, 0.22831684120279488, 0.044345003278233125, 0.18307194705511204, 0.05471115659118291, 0.150455680625753, 0.08837956064729546, 0.5239645381232517, 0.909574284399896, 0.0665542159316997, 0.7538566534046477, 0.050497574869210846, 0.08656727120436146, 0.023445302617847896, 0.08476378638760393, 0.5803680338057572, 0.025550793941133965, 0.04562641775202493, 0.19254348291354523, 0.1569548770669658, 0.2857157406688881, 0.1537483806289981, 0.47277627043416914, 0.04484327768345778, 0.04228080467297448, 0.7189231531255427, 0.027474770183141758, 0.20606077637356318, 0.018316513455427837, 0.027474770183141758, 0.10210189720225063, 0.13317638765510953, 0.6880780028847325, 0.07102740674939174, 0.008878425843673968, 0.48940378865700024, 0.09582731526151055, 0.1865210243482973, 0.03422404116482519, 0.19678823669774487, 0.924531206065353, 0.010750362861225035, 0.032251088583675105, 0.02150072572245007, 0.010750362861225035, 0.29175257618992606, 0.11033551972273568, 0.5262155556007394, 0.0381930645194085, 0.03288847222504621, 0.19067406083732136, 0.01834664502902061, 0.12711604055821424, 0.17494836509816084, 0.4894622798813713, 0.9086813718455302, 0.23843293218471337, 0.06938524481565733, 0.4428040169144677, 0.1564321883116638, 0.09335469302470259, 0.9417833804267113, 0.02478377316912398, 0.02478377316912398, 0.23766222462318992, 0.5251568511835003, 0.12074774315533036, 0.04599914024964966, 0.06899871037447448, 0.2207843019824089, 0.08815488316563808, 0.19431136409482988, 0.16651477931287192, 0.3301175354581102, 0.9341454608073291, 0.9661050295369634, 0.14432690022990027, 0.35297339730138655, 0.06275082618691316, 0.06902590880560448, 0.3733674158121333, 0.23465046171967652, 0.05090301021432136, 0.29051961927198044, 0.2843119350995022, 0.14029366229800766, 0.9412885632842778, 0.0571320285810993, 0.01142640571621986, 0.9141124572975888, 0.01142640571621986, 0.46690584494775494, 0.1092047083648687, 0.2529537632533183, 0.05905968921773511, 0.11254770964134428, 0.9355810280391754, 0.19978135612764827, 0.20101839238850058, 0.11380733599841264, 0.16143323204122662, 0.32410350034330554, 0.6055441818481297, 0.10912633665814352, 0.21825267331628703, 0.05135357019206754, 0.014978124639353031, 0.9278141487527419, 0.11039010993203367, 0.29477798586246357, 0.047310047113728715, 0.033966187671394976, 0.5131320494642884, 0.06044735436576328, 0.7253682523891594, 0.06044735436576328, 0.12089470873152657, 0.025346473702313535, 0.012673236851156768, 0.012673236851156768, 0.9378195269856009, 0.9569702349560023, 0.8520135699226579, 0.05680090466151052, 0.037867269774340344, 0.05680090466151052, 0.12479967016533693, 0.2637811210312803, 0.29498103857261454, 0.06523619122278976, 0.2510175184007345, 0.9656360426073515, 0.9507459857238, 0.005738909370566197, 0.00765187916075493, 0.013390788531321127, 0.02486860727245352, 0.19164268652099203, 0.1767866643100624, 0.5051047551716069, 0.04159686219060292, 0.08319372438120584, 0.031908088525487495, 0.015954044262743747, 0.8774724344509062, 0.015954044262743747, 0.047862132788231246, 0.26689415004545497, 0.05809940681261604, 0.34042621179267213, 0.2614473306567722, 0.07353206174721719, 0.07994742148624782, 0.8394479256056021, 0.07994742148624782, 0.19550082776863528, 0.022557787819457917, 0.19618439709649765, 0.17704445591635154, 0.40877445806169194, 0.03390582894079672, 0.9493632103423081, 0.021575460677513834, 0.021575460677513834, 0.8845938877780671, 0.04315092135502767, 0.02462958901711496, 0.7142580814963337, 0.22166630115403463, 0.04925917803422992, 0.22785495174422268, 0.12505170752292355, 0.5677194083862788, 0.03835941948555937, 0.04219536143411531, 0.01805025142458023, 0.9747135769273323, 0.17176272466833628, 0.014422824208791596, 0.07866995022977234, 0.1920857951443608, 0.5428226565854292, 0.9420049362154764, 0.03442489754188246, 0.03442489754188246, 0.9294722336308263, 0.007989725263206793, 0.03595376368443057, 0.8908543668475574, 0.03595376368443057, 0.03195890105282717, 0.1005282651163437, 0.08196920078717256, 0.15156569202156436, 0.09279532164585573, 0.5737844055102079, 0.026857626037505244, 0.08057287811251573, 0.8325864071626626, 0.08057287811251573, 0.09582696018336441, 0.051599132406426995, 0.7592443768374257, 0.02211391388846871, 0.07371304629489571, 0.3157167332846886, 0.0367570417129783, 0.2802724430614595, 0.084672471088825, 0.28289794604095797, 0.04614292634577542, 0.9690014532612838, 0.04077266465359592, 0.9377712870327062, 0.1960759513587462, 0.045546097876596095, 0.22886914182989537, 0.11796439350038389, 0.41128126382566277, 0.03273398782723232, 0.010911329275744109, 0.08729063420595287, 0.08183496956808081, 0.7856157078535758, 0.17248741390626984, 0.0627226959659163, 0.7160841122775444, 0.0209075653219721, 0.03136134798295815, 0.1870198422360212, 0.06800721535855317, 0.6673208007058029, 0.02975315671936701, 0.0467549605590053, 0.0495255232433276, 0.0247627616216638, 0.9162221800015606, 0.03678559563417864, 0.07357119126835727, 0.14714238253671455, 0.6621407214152154, 0.07357119126835727, 0.025605769093406776, 0.8705961491758304, 0.025605769093406776, 0.05121153818681355, 0.025605769093406776, 0.35146706125770627, 0.17790308038970318, 0.21813836686364935, 0.07731486420483774, 0.17514183523953042, 0.04415702009131038, 0.9272974219175181, 0.9261091595446349, 0.8150376463810807, 0.03918450222985965, 0.07053210401374736, 0.023510701337915788, 0.047021402675831575, 0.9679510341792276, 0.018779279220724723, 0.018779279220724723, 0.018779279220724723, 0.9201846818155114, 0.8846241854277915, 0.9783943551963813, 0.9235922473652717, 0.05432895572736893, 0.9465462182950768, 0.968132100086715, 0.9709837803679052, 0.9178692715337611, 0.944495339629578, 0.011328392462636732, 0.022656784925273465, 0.011328392462636732, 0.9629133593241223, 0.9535625597021983, 0.16073505667280177, 0.04047285599674865, 0.2243352589534068, 0.29603003243336157, 0.27810633906337284, 0.2913452785354279, 0.06424247921284097, 0.14426381296918675, 0.13186614154214726, 0.3685489596947193, 0.0421710348455509, 0.028114023230367265, 0.0421710348455509, 0.8715347201413852, 0.23476594497031875, 0.13395866600611847, 0.30073043959454376, 0.11670641356593656, 0.2141308979340227, 0.9175256214455949, 0.041705710065708855, 0.17261597119603575, 0.05894203894498782, 0.6946740304230707, 0.02526087383356621, 0.04631160202820472, 0.9541538583439547, 0.12166097345027273, 0.352568535304872, 0.1340753584962189, 0.1266267274686512, 0.26318496297405936, 0.19013852838269393, 0.07328255781416329, 0.07262235459061227, 0.12279779958048984, 0.5420268465353879, 0.5053912748860327, 0.0673855033181377, 0.23584926161348194, 0.05294575260710819, 0.13637542338194533, 0.052570794839180375, 0.9199889096856566, 0.18686367035832385, 0.09291563719474666, 0.05884657022333955, 0.17860450260767968, 0.48316131341268265, 0.9547984686936227, 0.027168710519684842, 0.027168710519684842, 0.9237361576692846, 0.933242532798992, 0.9466873670804677, 0.9902619339052986, 0.02032897025004768, 0.9757905720022885], \"Term\": [\"2\", \"2\", \"2\", \"2\", \"2\", \"2011\", \"250\", \"a.m.\", \"accent\", \"accessory\", \"accessory\", \"accessory\", \"alex\", \"alfredo\", \"alfredo\", \"amaze\", \"amaze\", \"amaze\", \"amaze\", \"amaze\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"apartment\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"appointment\", \"arepa\", \"arrogant\", \"art\", \"art\", \"art\", \"art\", \"art\", \"asado\", \"ask\", \"ask\", \"ask\", \"ask\", \"ask\", \"audience\", \"audience\", \"audience\", \"aunt\", \"avec\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"awesome\", \"ayce\", \"ayce\", \"ayce\", \"ayce\", \"baba\", \"bacon\", \"bacon\", \"bacon\", \"bacon\", \"bacon\", \"bar\", \"bar\", \"bar\", \"bar\", \"bar\", \"bare\", \"beef\", \"beef\", \"beef\", \"beef\", \"beef\", \"beer\", \"beer\", \"beer\", \"beer\", \"beer\", \"benedict\", \"benedict\", \"benedict\", \"benedict\", \"bike\", \"bike\", \"bike\", \"bike\", \"bike\", \"biscuit\", \"biscuit\", \"biscuit\", \"biscuit\", \"bisque\", \"bisque\", \"bite\", \"bite\", \"bite\", \"bite\", \"bite\", \"boba\", \"boba\", \"boba\", \"boba\", \"boba\", \"bouchon\", \"bouchon\", \"brake\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"breakfast\", \"brewery\", \"brewery\", \"bridal\", \"britney\", \"brunch\", \"brunch\", \"brunch\", \"brunch\", \"brunch\", \"brussel\", \"brussel\", \"burger\", \"burger\", \"burger\", \"burger\", \"burger\", \"cajun\", \"call\", \"call\", \"call\", \"call\", \"call\", \"canal\", \"canal\", \"canal\", \"canal\", \"capacity\", \"car\", \"car\", \"car\", \"car\", \"car\", \"care\", \"care\", \"care\", \"care\", \"care\", \"carl\", \"carl\", \"cat\", \"cat\", \"cat\", \"category\", \"category\", \"cave\", \"cesar\", \"chang\", \"chang\", \"check\", \"check\", \"check\", \"check\", \"check\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"cheese\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chicken\", \"chocolate\", \"chocolate\", \"chocolate\", \"chocolate\", \"chocolate\", \"chorizo\", \"chorizo\", \"church\", \"church\", \"class\", \"class\", \"class\", \"class\", \"class\", \"collection\", \"collection\", \"collection\", \"collection\", \"collection\", \"come\", \"come\", \"come\", \"come\", \"come\", \"comic\", \"comic\", \"comic\", \"comme\", \"company\", \"company\", \"company\", \"company\", \"company\", \"concern\", \"concern\", \"concern\", \"concern\", \"concern\", \"concoction\", \"connect\", \"connect\", \"connect\", \"connect\", \"connect\", \"contractor\", \"contractor\", \"cowboy\", \"cox\", \"cox\", \"cox\", \"crab\", \"crab\", \"crab\", \"crab\", \"crab\", \"cream\", \"cream\", \"cream\", \"cream\", \"cream\", \"crispy\", \"crispy\", \"crispy\", \"crispy\", \"crispy\", \"crust\", \"crust\", \"crust\", \"crust\", \"crust\", \"cupcake\", \"cupcake\", \"cupcake\", \"cupcake\", \"cupcake\", \"curly\", \"curly\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"dans\", \"day\", \"day\", \"day\", \"day\", \"day\", \"de\", \"de\", \"de\", \"de\", \"de\", \"decently\", \"decently\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"definitely\", \"del\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"delicious\", \"dental\", \"dental\", \"dental\", \"dentist\", \"dentist\", \"dentist\", \"dentist\", \"depot\", \"depot\", \"des\", \"des\", \"des\", \"dim\", \"dim\", \"dim\", \"dim\", \"dim\", \"dish\", \"dish\", \"dish\", \"dish\", \"dish\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"dog\", \"dog\", \"dog\", \"dog\", \"dog\", \"dolphin\", \"domino\", \"dr\", \"dr\", \"dr\", \"dr\", \"dr\", \"dragon\", \"dragon\", \"dragon\", \"dragon\", \"drama\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"driver\", \"driver\", \"driver\", \"driver\", \"driver\", \"drywall\", \"earl\", \"eat\", \"eat\", \"eat\", \"eat\", \"eat\", \"egg\", \"egg\", \"egg\", \"egg\", \"egg\", \"email\", \"email\", \"email\", \"email\", \"email\", \"en\", \"en\", \"en\", \"enchilada\", \"enchiladas\", \"enchiladas\", \"enthusiastic\", \"enthusiastic\", \"est\", \"est\", \"et\", \"et\", \"et\", \"exam\", \"exam\", \"exhaust\", \"exhibit\", \"exhibit\", \"exhibit\", \"experience\", \"experience\", \"experience\", \"experience\", \"experience\", \"eyebrow\", \"eyebrow\", \"feel\", \"feel\", \"feel\", \"feel\", \"feel\", \"festival\", \"fi\", \"fi\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fish\", \"fish\", \"fish\", \"fish\", \"fish\", \"fix\", \"fix\", \"fix\", \"fix\", \"fix\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"flavor\", \"food\", \"food\", \"food\", \"food\", \"food\", \"ford\", \"ford\", \"ford\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"friendly\", \"frill\", \"frill\", \"frog\", \"froyo\", \"fry\", \"fry\", \"fry\", \"fry\", \"fry\", \"gabi\", \"game\", \"game\", \"game\", \"game\", \"game\", \"get\", \"get\", \"get\", \"get\", \"get\", \"give\", \"give\", \"give\", \"give\", \"give\", \"gluten\", \"gluten\", \"gluten\", \"gluten\", \"go\", \"go\", \"go\", \"go\", \"go\", \"good\", \"good\", \"good\", \"good\", \"good\", \"great\", \"great\", \"great\", \"great\", \"great\", \"greek\", \"greek\", \"greek\", \"greek\", \"greek\", \"gyoza\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"haircut\", \"haircut\", \"haircut\", \"haircut\", \"haircut\", \"hakka\", \"hall\", \"halo\", \"harness\", \"harness\", \"heartbeat\", \"help\", \"help\", \"help\", \"help\", \"help\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"helpful\", \"hoagie\", \"horchata\", \"horribly\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"hotel\", \"humor\", \"ice\", \"ice\", \"ice\", \"ice\", \"ice\", \"ikea\", \"installer\", \"instruct\", \"instructor\", \"instructor\", \"instructor\", \"instructor\", \"insurance\", \"insurance\", \"insurance\", \"insurance\", \"ipa\", \"ipa\", \"ipa\", \"ipa\", \"ipad\", \"iphone\", \"izakaya\", \"izakaya\", \"j'ai\", \"jalape\\u00f1os\", \"jamba\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"japanese\", \"je\", \"job\", \"job\", \"job\", \"job\", \"job\", \"keg\", \"keg\", \"kim\", \"kim\", \"kim\", \"kim\", \"knot\", \"know\", \"know\", \"know\", \"know\", \"know\", \"korean\", \"korean\", \"korean\", \"korean\", \"korean\", \"la\", \"la\", \"la\", \"la\", \"la\", \"large\", \"large\", \"large\", \"large\", \"large\", \"laser\", \"laser\", \"lawyer\", \"le\", \"le\", \"le\", \"le\", \"le\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"lens\", \"les\", \"les\", \"les\", \"library\", \"library\", \"like\", \"like\", \"like\", \"like\", \"like\", \"little\", \"little\", \"little\", \"little\", \"little\", \"lobster\", \"lobster\", \"lobster\", \"lobster\", \"lobster\", \"local\", \"local\", \"local\", \"local\", \"local\", \"look\", \"look\", \"look\", \"look\", \"look\", \"love\", \"love\", \"love\", \"love\", \"love\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"lunch\", \"magician\", \"maintenance\", \"maintenance\", \"maintenance\", \"maintenance\", \"maintenance\", \"mais\", \"mary\", \"mary\", \"mary\", \"massage\", \"massage\", \"massage\", \"massage\", \"massage\", \"mayo\", \"mayo\", \"mayo\", \"mayo\", \"meal\", \"meal\", \"meal\", \"meal\", \"meal\", \"mein\", \"mein\", \"mein\", \"mein\", \"menu\", \"menu\", \"menu\", \"menu\", \"menu\", \"mex\", \"mex\", \"mince\", \"mint\", \"mint\", \"mint\", \"mint\", \"mint\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"mocha\", \"mocha\", \"mocha\", \"mocha\", \"month\", \"month\", \"month\", \"month\", \"month\", \"muffin\", \"muffin\", \"muffin\", \"muffin\", \"muffin\", \"musician\", \"nail\", \"nail\", \"nail\", \"nail\", \"nail\", \"need\", \"need\", \"need\", \"need\", \"need\", \"new\", \"new\", \"new\", \"new\", \"new\", \"nice\", \"nice\", \"nice\", \"nice\", \"nice\", \"night\", \"night\", \"night\", \"night\", \"night\", \"nigiri\", \"noir\", \"noir\", \"noir\", \"omelette\", \"omelette\", \"omelette\", \"omelette\", \"onion\", \"onion\", \"onion\", \"onion\", \"onion\", \"open\", \"open\", \"open\", \"open\", \"open\", \"order\", \"order\", \"order\", \"order\", \"order\", \"ou\", \"oxtail\", \"oxtail\", \"oxtail\", \"pad\", \"pad\", \"pad\", \"pad\", \"pad\", \"pancake\", \"pancake\", \"pancake\", \"pancake\", \"pancake\", \"pas\", \"pas\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pear\", \"pear\", \"pear\", \"pecan\", \"pedicure\", \"pedicure\", \"pedicure\", \"pedicure\", \"pedicure\", \"penne\", \"people\", \"people\", \"people\", \"people\", \"people\", \"pepperoni\", \"pepperoni\", \"pepperoni\", \"pepperoni\", \"personalize\", \"pet\", \"pet\", \"pet\", \"pet\", \"pho\", \"pho\", \"pho\", \"pho\", \"phone\", \"phone\", \"phone\", \"phone\", \"phone\", \"pig\", \"pig\", \"pig\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"pizza\", \"pizzeria\", \"place\", \"place\", \"place\", \"place\", \"place\", \"poach\", \"poach\", \"port\", \"possibility\", \"potato\", \"potato\", \"potato\", \"potato\", \"potato\", \"poutine\", \"poutine\", \"poutine\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"pretty\", \"price\", \"price\", \"price\", \"price\", \"price\", \"prickly\", \"professional\", \"professional\", \"professional\", \"professional\", \"professional\", \"professionally\", \"que\", \"que\", \"queso\", \"queso\", \"queso\", \"queso\", \"queso\", \"quinoa\", \"ragu\", \"ragu\", \"ranchero\", \"rancheros\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"recommend\", \"removal\", \"rend\", \"rend\", \"rend\", \"rend\", \"repair\", \"repair\", \"repair\", \"repair\", \"repair\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"restaurant\", \"review\", \"review\", \"review\", \"review\", \"review\", \"robert\", \"robert\", \"roll\", \"roll\", \"roll\", \"roll\", \"roll\", \"room\", \"room\", \"room\", \"room\", \"room\", \"salad\", \"salad\", \"salad\", \"salad\", \"salad\", \"salmon\", \"salmon\", \"salmon\", \"salmon\", \"salmon\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"salsa\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sandwich\", \"sashimi\", \"sashimi\", \"sashimi\", \"sashimi\", \"sashimi\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"sauce\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scott\", \"seat\", \"seat\", \"seat\", \"seat\", \"seat\", \"seaweed\", \"seaweed\", \"seaweed\", \"selection\", \"selection\", \"selection\", \"selection\", \"selection\", \"service\", \"service\", \"service\", \"service\", \"service\", \"sever\", \"severe\", \"shop\", \"shop\", \"shop\", \"shop\", \"shop\", \"sit\", \"sit\", \"sit\", \"sit\", \"sit\", \"skinny\", \"slider\", \"slider\", \"slider\", \"slider\", \"small\", \"small\", \"small\", \"small\", \"small\", \"sont\", \"staff\", \"staff\", \"staff\", \"staff\", \"staff\", \"steak\", \"steak\", \"steak\", \"steak\", \"steak\", \"storage\", \"store\", \"store\", \"store\", \"store\", \"store\", \"study\", \"study\", \"study\", \"study\", \"stylist\", \"stylist\", \"stylist\", \"stylist\", \"sucker\", \"sum\", \"sum\", \"sum\", \"sum\", \"super\", \"super\", \"super\", \"super\", \"super\", \"sur\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sushi\", \"sweet\", \"sweet\", \"sweet\", \"sweet\", \"sweet\", \"syrup\", \"syrup\", \"syrup\", \"syrup\", \"syrup\", \"table\", \"table\", \"table\", \"table\", \"table\", \"taiwanese\", \"taiwanese\", \"taiwanese\", \"take\", \"take\", \"take\", \"take\", \"take\", \"tamale\", \"tamale\", \"tan\", \"tan\", \"tan\", \"tan\", \"tart\", \"tart\", \"tart\", \"tart\", \"taste\", \"taste\", \"taste\", \"taste\", \"taste\", \"teacher\", \"teacher\", \"tell\", \"tell\", \"tell\", \"tell\", \"tell\", \"teller\", \"tenant\", \"tenant\", \"tenant\", \"thai\", \"thai\", \"thai\", \"thai\", \"thai\", \"thank\", \"thank\", \"thank\", \"thank\", \"thank\", \"therapist\", \"therapist\", \"therapist\", \"therapist\", \"thin\", \"thin\", \"thin\", \"thin\", \"thin\", \"think\", \"think\", \"think\", \"think\", \"think\", \"thread\", \"thread\", \"tim\", \"tim\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tire\", \"tire\", \"tire\", \"tire\", \"tire\", \"toast\", \"toast\", \"toast\", \"toast\", \"toast\", \"tomato\", \"tomato\", \"tomato\", \"tomato\", \"tomato\", \"tone\", \"tone\", \"tone\", \"tooth\", \"tooth\", \"tooth\", \"tooth\", \"tooth\", \"trail\", \"trail\", \"trail\", \"trail\", \"trail\", \"try\", \"try\", \"try\", \"try\", \"try\", \"tr\\u00e8s\", \"tr\\u00e8s\", \"tucson\", \"tuna\", \"tuna\", \"tuna\", \"tuna\", \"tuna\", \"umami\", \"un\", \"un\", \"un\", \"un\", \"und\", \"une\", \"uni\", \"uni\", \"unorganized\", \"uptown\", \"urban\", \"vaccination\", \"verify\", \"vet\", \"vet\", \"vet\", \"vet\", \"vous\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"want\", \"want\", \"want\", \"want\", \"want\", \"wax\", \"wax\", \"wax\", \"wax\", \"well\", \"well\", \"well\", \"well\", \"well\", \"wi\", \"wi\", \"wing\", \"wing\", \"wing\", \"wing\", \"wing\", \"wire\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"wonderful\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worth\", \"worth\", \"worth\", \"worth\", \"worth\", \"x\", \"x\", \"year\", \"year\", \"year\", \"year\", \"year\", \"yellowtail\", \"yoga\", \"yoga\", \"yoga\", \"yolk\", \"yuck\", \"yuk\", \"\\u00e0\", \"\\u00e0\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el17941405764580758569226187835\", ldavis_el17941405764580758569226187835_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el17941405764580758569226187835\", ldavis_el17941405764580758569226187835_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el17941405764580758569226187835\", ldavis_el17941405764580758569226187835_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.020069 -0.017162       1        1  26.495003\n",
       "1      0.073833  0.066331       2        1  11.097117\n",
       "2      0.069470 -0.052837       3        1  23.424228\n",
       "3     -0.072556 -0.030086       4        1  12.588055\n",
       "4     -0.090816  0.033754       5        1  26.395597, topic_info=         Term         Freq        Total Category  logprob  loglift\n",
       "668      love  2269.000000  2269.000000  Default  30.0000  30.0000\n",
       "144      food  4966.000000  4966.000000  Default  29.0000  29.0000\n",
       "205     great  4294.000000  4294.000000  Default  28.0000  28.0000\n",
       "367     order  3619.000000  3619.000000  Default  27.0000  27.0000\n",
       "1136  chicken  1474.000000  1474.000000  Default  26.0000  26.0000\n",
       "...       ...          ...          ...      ...      ...      ...\n",
       "131      well   632.741137  2956.135738   Topic5  -5.3215  -0.2096\n",
       "668      love   568.133725  2269.840490   Topic5  -5.4292  -0.0531\n",
       "43       good   671.375695  4818.050203   Topic5  -5.2623  -0.6388\n",
       "367     order   545.261282  3619.643123   Topic5  -5.4703  -0.5609\n",
       "144      food   502.873072  4966.513069   Topic5  -5.5512  -0.9582\n",
       "\n",
       "[469 rows x 6 columns], token_table=       Topic      Freq  Term\n",
       "term                        \n",
       "398        1  0.263313     2\n",
       "398        2  0.038638     2\n",
       "398        3  0.237554     2\n",
       "398        4  0.179597     2\n",
       "398        5  0.280486     2\n",
       "...      ...       ...   ...\n",
       "8125       3  0.933243  yolk\n",
       "8647       3  0.946687  yuck\n",
       "11055      5  0.990262   yuk\n",
       "3373       1  0.020329     à\n",
       "3373       4  0.975791     à\n",
       "\n",
       "[1199 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use pyLDAvis (or a ploting tool of your choice) to visualize your results \n",
    "\n",
    "# YOUR CODE HERE\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "# sort_topics=False => Retain gensim's topic ordering so that the\n",
    "# output of lda.print_topics() can be matched with the pyLDAvis\n",
    "# diagram.\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, id2word, sort_topics=False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f44a26c754500ff0bf585296075bf754",
     "grade": false,
     "grade_id": "cell-bf9e63d9645bba84",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "#### 3. In markdown, write 1-2 paragraphs of analysis on the results of your topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Goals\n",
    "\n",
    "Complete one of more of these to push your score towards a three: \n",
    "* Incorporate named entity recognition into your analysis\n",
    "* Compare vectorization methods in the classification section\n",
    "* Analyze more (or all) of the yelp dataset - this one is v. hard. \n",
    "* Use a generator object on the reviews file - this would help you with the analyzing the whole dataset.\n",
    "* Incorporate any of the other yelp dataset entities in your analysis (business, users, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "u4-s1-nlp"
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP (Python3)",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
